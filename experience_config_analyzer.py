#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½“éªŒæ¨¡å¼é…ç½®[x,y,z]å¯¹TileMatchæŒ‡æ ‡å½±å“çš„æ·±åº¦åˆ†æå·¥å…·
å‡çº§ç‰ˆï¼šä½ç½®ç‹¬ç«‹æ•ˆåº”ã€äº¤äº’æ•ˆåº”ã€åŠ¨æ€å½±å“ã€æœºåˆ¶åˆ†æ

ä½œè€…: Claude Code Assistant
å‡çº§æ—¶é—´: 2025-01-27
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.preprocessing import StandardScaler
from scipy import stats
from scipy.stats import pearsonr, spearmanr
import warnings
import os
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import json

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œæ ·å¼
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")
warnings.filterwarnings('ignore')

class ExperienceConfigAnalyzer:
    """ä½“éªŒæ¨¡å¼é…ç½®åˆ†æå™¨ - æ·±åº¦åˆ†æ[x,y,z]ä½ç½®å½±å“æœºåˆ¶"""

    def __init__(self, csv_path: str = None, csv_directory: str = None):
        """åˆå§‹åŒ–åˆ†æå™¨

        Args:
            csv_path: å•ä¸ªCSVæ•°æ®æ–‡ä»¶è·¯å¾„
            csv_directory: CSVæ–‡ä»¶ç›®å½•è·¯å¾„ï¼Œç”¨äºæ‰¹é‡å¤„ç†å¤šä¸ªæ–‡ä»¶
        """
        self.csv_path = csv_path
        self.csv_directory = csv_directory or self._find_csv_directory()
        self.data = None
        self.features = None
        self.csv_files = []
        self.target_metrics = [
            'DifficultyScore', 'PeakDockCount', 'PressureValueMean',
            'PressureValueMax', 'PressureValueStdDev', 'FinalDifficulty',
            'TotalMoves', 'InitialMinCost', 'DifficultyPosition'
        ]
        self.results = {}

    def _find_csv_directory(self) -> str:
        """è‡ªåŠ¨æŸ¥æ‰¾CSVæ–‡ä»¶ç›®å½•"""
        current_dir = Path(__file__).parent

        # ä¼˜å…ˆæŸ¥æ‰¾analysis_chartsç›®å½•
        analysis_charts_dir = current_dir / "BattleAnalysisResults" / "analysis_charts"
        if analysis_charts_dir.exists():
            csv_files = list(analysis_charts_dir.glob("*.csv"))
            if csv_files:
                print(f"ğŸ” æ£€æµ‹åˆ°analysis_chartsç›®å½•ï¼ŒåŒ…å«{len(csv_files)}ä¸ªCSVæ–‡ä»¶")
                return str(analysis_charts_dir)

        # æŸ¥æ‰¾BattleAnalysisResultsæ ¹ç›®å½•
        results_dir = current_dir / "BattleAnalysisResults"
        if results_dir.exists():
            csv_files = list(results_dir.glob("*.csv"))
            if csv_files:
                print(f"ğŸ” æ£€æµ‹åˆ°BattleAnalysisResultsç›®å½•ï¼ŒåŒ…å«{len(csv_files)}ä¸ªCSVæ–‡ä»¶")
                return str(results_dir)

        # æŸ¥æ‰¾å½“å‰ç›®å½•
        csv_files = list(current_dir.glob("*.csv"))
        if csv_files:
            print(f"ğŸ” æ£€æµ‹åˆ°å½“å‰ç›®å½•ï¼ŒåŒ…å«{len(csv_files)}ä¸ªCSVæ–‡ä»¶")
            return str(current_dir)

        raise FileNotFoundError("æœªæ‰¾åˆ°åŒ…å«CSVæ–‡ä»¶çš„ç›®å½•ï¼Œè¯·ç¡®ä¿æ•°æ®æ–‡ä»¶å­˜åœ¨")

    def load_and_preprocess(self) -> 'ExperienceConfigAnalyzer':
        """åŠ è½½æ•°æ®å¹¶è¿›è¡Œé¢„å¤„ç†"""
        print("ğŸ“Š åŠ è½½å’Œé¢„å¤„ç†æ•°æ®...")

        if self.csv_path:
            # å•æ–‡ä»¶æ¨¡å¼
            self.data = pd.read_csv(self.csv_path, encoding='utf-8')
            print(f"âœ… å•æ–‡ä»¶æ•°æ®åŠ è½½å®Œæˆ: {len(self.data)}æ¡è®°å½•")
        else:
            # å¤šæ–‡ä»¶æ¨¡å¼ - æ‰¹é‡åŠ è½½
            self._load_multiple_csv_files()

        # è§£æä½“éªŒæ¨¡å¼é…ç½®
        self._parse_experience_config()

        # ç”Ÿæˆç‰¹å¾å·¥ç¨‹
        self._create_features()

        # æ•°æ®æ¸…æ´—
        self._clean_data()

        print(f"ğŸ“ˆ é¢„å¤„ç†å®Œæˆ: {len(self.data)}æ¡æœ‰æ•ˆè®°å½•, {len(self.features.columns)}ä¸ªç‰¹å¾")
        return self

    def _load_multiple_csv_files(self):
        """æ‰¹é‡åŠ è½½å¤šä¸ªCSVæ–‡ä»¶ï¼Œä½¿ç”¨æœ€ç®€å•çš„æ–¹æ¡ˆ"""
        csv_dir = Path(self.csv_directory)
        self.csv_files = sorted(list(csv_dir.glob("*.csv")))

        if not self.csv_files:
            raise FileNotFoundError(f"åœ¨ç›®å½• {csv_dir} ä¸­æœªæ‰¾åˆ°CSVæ–‡ä»¶")

        print(f"ğŸ” å‘ç°{len(self.csv_files)}ä¸ªCSVæ–‡ä»¶ï¼Œå‡†å¤‡æ‰¹é‡åŠ è½½...")

        # ç®€å•ç›´æ¥åŠ è½½ï¼Œä¸€ä¸ªæ–‡ä»¶ä¸€ä¸ªæ–‡ä»¶å¤„ç†
        data_list = []
        total_rows = 0

        for i, csv_file in enumerate(self.csv_files):
            print(f"   æ­£åœ¨åŠ è½½ {csv_file.name}... ({i+1}/{len(self.csv_files)})")

            try:
                # ç›´æ¥è¯»å–ï¼Œä¸åˆ†å—
                file_data = pd.read_csv(csv_file, encoding='utf-8')
                data_list.append(file_data)
                total_rows += len(file_data)
                print(f"     âœ… {csv_file.name}: {len(file_data)}è¡Œ")

            except Exception as e:
                print(f"     âŒ åŠ è½½{csv_file.name}å¤±è´¥: {str(e)}")
                continue

        if not data_list:
            raise ValueError("æ‰€æœ‰CSVæ–‡ä»¶éƒ½åŠ è½½å¤±è´¥")

        # åˆå¹¶æ‰€æœ‰æ•°æ®
        print("ğŸ”— åˆå¹¶æ‰€æœ‰æ•°æ®...")
        self.data = pd.concat(data_list, ignore_index=True)

        print(f"âœ… æ‰¹é‡æ•°æ®åŠ è½½å®Œæˆ: æ€»è®¡{total_rows}æ¡è®°å½•ï¼Œåˆå¹¶å{len(self.data)}æ¡è®°å½•")

    def _parse_experience_config(self):
        """è§£æä½“éªŒæ¨¡å¼é…ç½®[1,2,3]æ ¼å¼ï¼Œä½¿ç”¨æœ€ç®€å•çš„å®ç°"""
        # æ£€æŸ¥ExperienceModeåˆ—æ˜¯å¦å­˜åœ¨
        if 'ExperienceMode' not in self.data.columns:
            print("âš ï¸ æœªæ‰¾åˆ°ExperienceModeåˆ—ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            self.data['pos1'] = 1
            self.data['pos2'] = 2
            self.data['pos3'] = 3
            return

        print("ğŸ”§ è§£æä½“éªŒæ¨¡å¼é…ç½®...")

        # ä½¿ç”¨æœ€ç®€å•çš„è§£ææ–¹å¼ï¼Œé¿å…å¤æ‚çš„applyæ“ä½œ
        pos1_list = []
        pos2_list = []
        pos3_list = []

        for config_str in self.data['ExperienceMode']:
            try:
                if pd.isna(config_str):
                    pos1_list.append(1)
                    pos2_list.append(2)
                    pos3_list.append(3)
                    continue

                # ç®€å•å­—ç¬¦ä¸²å¤„ç†
                clean_str = str(config_str).strip('[]"()')
                numbers = [int(x.strip()) for x in clean_str.split(',')]

                if len(numbers) >= 3:
                    pos1_list.append(numbers[0])
                    pos2_list.append(numbers[1])
                    pos3_list.append(numbers[2])
                else:
                    pos1_list.append(1)
                    pos2_list.append(2)
                    pos3_list.append(3)

            except:
                pos1_list.append(1)
                pos2_list.append(2)
                pos3_list.append(3)

        # ç›´æ¥èµ‹å€¼ï¼Œä¸ä½¿ç”¨apply
        self.data['pos1'] = pos1_list
        self.data['pos2'] = pos2_list
        self.data['pos3'] = pos3_list

        # éªŒè¯è§£æç»“æœ
        unique_configs = len(self.data[['pos1', 'pos2', 'pos3']].drop_duplicates())
        print(f"   è§£æå®Œæˆï¼šå‘ç°{unique_configs}ç§ä¸åŒé…ç½®")

    def _create_features(self):
        """åˆ›å»ºç‰¹å¾å·¥ç¨‹"""
        # åŸºç¡€ä½ç½®ç‰¹å¾
        features_df = self.data[['pos1', 'pos2', 'pos3']].copy()

        # ç»Ÿè®¡ç‰¹å¾
        features_df['config_mean'] = (self.data['pos1'] + self.data['pos2'] + self.data['pos3']) / 3
        features_df['config_std'] = np.sqrt(((self.data['pos1'] - features_df['config_mean'])**2 +
                                            (self.data['pos2'] - features_df['config_mean'])**2 +
                                            (self.data['pos3'] - features_df['config_mean'])**2) / 3)
        features_df['config_range'] = np.maximum.reduce([self.data['pos1'], self.data['pos2'], self.data['pos3']]) - \
                                     np.minimum.reduce([self.data['pos1'], self.data['pos2'], self.data['pos3']])
        features_df['config_sum'] = self.data['pos1'] + self.data['pos2'] + self.data['pos3']

        # äº¤äº’ç‰¹å¾
        features_df['pos1_pos2'] = self.data['pos1'] * self.data['pos2']
        features_df['pos1_pos3'] = self.data['pos1'] * self.data['pos3']
        features_df['pos2_pos3'] = self.data['pos2'] * self.data['pos3']
        features_df['pos_product'] = self.data['pos1'] * self.data['pos2'] * self.data['pos3']

        # åºåˆ—æ¨¡å¼ç‰¹å¾
        features_df['is_increasing'] = ((self.data['pos1'] <= self.data['pos2']) &
                                      (self.data['pos2'] <= self.data['pos3'])).astype(int)
        features_df['is_decreasing'] = ((self.data['pos1'] >= self.data['pos2']) &
                                      (self.data['pos2'] >= self.data['pos3'])).astype(int)
        features_df['is_uniform'] = ((self.data['pos1'] == self.data['pos2']) &
                                   (self.data['pos2'] == self.data['pos3'])).astype(int)

        # æå€¼ç‰¹å¾
        features_df['has_extreme_low'] = ((self.data['pos1'] == 1) | (self.data['pos2'] == 1) | (self.data['pos3'] == 1)).astype(int)
        features_df['has_extreme_high'] = ((self.data['pos1'] == 9) | (self.data['pos2'] == 9) | (self.data['pos3'] == 9)).astype(int)

        self.features = features_df

    def _clean_data(self):
        """æ•°æ®æ¸…æ´—ï¼Œå¢å¼ºè¾¹ç•Œæ¡ä»¶æ£€æŸ¥"""
        initial_count = len(self.data)
        print(f"ğŸ§¹ å¼€å§‹æ•°æ®æ¸…æ´—ï¼Œåˆå§‹è®°å½•æ•°: {initial_count}")

        # ä¿å­˜åŸå§‹ç´¢å¼•ç”¨äºç‰¹å¾æ•°æ®åŒæ­¥
        original_indices = self.data.index.copy()

        # æ£€æŸ¥GameCompletedåˆ—æ˜¯å¦å­˜åœ¨
        if 'GameCompleted' in self.data.columns:
            # ç§»é™¤GameCompleted=Falseçš„è®°å½•
            completed_mask = (self.data['GameCompleted'] == True) | (self.data['GameCompleted'] == 'True')
            self.data = self.data[completed_mask].copy()
            print(f"   å®Œæˆæ¸¸æˆè¿‡æ»¤: {len(self.data)}æ¡è®°å½•")
        else:
            print("   âš ï¸ æœªæ‰¾åˆ°GameCompletedåˆ—ï¼Œè·³è¿‡å®ŒæˆçŠ¶æ€è¿‡æ»¤")

        # ç§»é™¤ç›®æ ‡æŒ‡æ ‡ä¸ºç©ºå€¼çš„è®°å½•
        metrics_found = []
        for metric in self.target_metrics:
            if metric in self.data.columns:
                metrics_found.append(metric)
                before_count = len(self.data)
                self.data = self.data[self.data[metric].notna()].copy()
                if before_count != len(self.data):
                    print(f"   {metric}ç©ºå€¼è¿‡æ»¤: {len(self.data)}æ¡è®°å½•")

        if not metrics_found:
            print("   âš ï¸ æœªæ‰¾åˆ°ä»»ä½•ç›®æ ‡æŒ‡æ ‡åˆ—ï¼Œæ•°æ®å¯èƒ½å­˜åœ¨é—®é¢˜")
        else:
            print(f"   æ‰¾åˆ°{len(metrics_found)}ä¸ªæœ‰æ•ˆæŒ‡æ ‡: {', '.join(metrics_found)}")

        # ç§»é™¤é…ç½®è§£æå¼‚å¸¸çš„è®°å½•
        invalid_config_mask = (
            (self.data['pos1'] < 1) | (self.data['pos1'] > 9) |
            (self.data['pos2'] < 1) | (self.data['pos2'] > 9) |
            (self.data['pos3'] < 1) | (self.data['pos3'] > 9)
        )
        if invalid_config_mask.any():
            invalid_count = invalid_config_mask.sum()
            self.data = self.data[~invalid_config_mask].copy()
            print(f"   é…ç½®å¼‚å¸¸è¿‡æ»¤: ç§»é™¤{invalid_count}æ¡ï¼Œå‰©ä½™{len(self.data)}æ¡è®°å½•")

        # å®‰å…¨çš„ç‰¹å¾æ•°æ®é‡æ–°ç´¢å¼•
        try:
            # æ‰¾åˆ°ç‰¹å¾æ•°æ®å’Œæ¸…æ´—åæ•°æ®çš„äº¤é›†ç´¢å¼•
            valid_indices = self.data.index.intersection(self.features.index)

            if len(valid_indices) == len(self.data):
                # å®Œç¾åŒ¹é…ï¼Œç›´æ¥é‡æ–°ç´¢å¼•
                self.features = self.features.loc[valid_indices].copy()
            else:
                # ä¸å®Œç¾åŒ¹é…ï¼Œé‡æ–°ç”Ÿæˆç‰¹å¾æ•°æ®ä»¥ä¿è¯ä¸€è‡´æ€§
                print(f"   âš ï¸ ç‰¹å¾æ•°æ®ç´¢å¼•ä¸åŒ¹é…ï¼Œé‡æ–°ç”Ÿæˆç‰¹å¾")
                self._create_features()

        except (KeyError, IndexError) as e:
            # ç´¢å¼•ä¸¥é‡ä¸åŒ¹é…ï¼Œé‡æ–°ç”Ÿæˆ
            print(f"   âŒ ç‰¹å¾æ•°æ®ç´¢å¼•é”™è¯¯ï¼Œé‡æ–°ç”Ÿæˆ: {str(e)}")
            self._create_features()

        final_count = len(self.data)
        filtered_ratio = (initial_count - final_count) / initial_count * 100 if initial_count > 0 else 0
        print(f"ğŸ§¹ æ•°æ®æ¸…æ´—å®Œæˆ: {initial_count} -> {final_count}æ¡è®°å½• (è¿‡æ»¤{filtered_ratio:.1f}%)")

        # æ•°æ®è´¨é‡æ£€æŸ¥
        if final_count < initial_count * 0.1:  # å¦‚æœè¿‡æ»¤æ‰è¶…è¿‡90%çš„æ•°æ®
            print(f"âš ï¸ è­¦å‘Š: è¿‡æ»¤æ¯”ä¾‹è¿‡é«˜({filtered_ratio:.1f}%)ï¼Œè¯·æ£€æŸ¥æ•°æ®è´¨é‡")

        if final_count < 100:  # å¦‚æœæœ€ç»ˆæ•°æ®é‡è¿‡å°‘
            print(f"âš ï¸ è­¦å‘Š: æœ‰æ•ˆæ•°æ®é‡è¿‡å°‘({final_count}æ¡)ï¼Œåˆ†æç»“æœå¯èƒ½ä¸å¯é ")

    def correlation_analysis(self) -> Dict:
        """åŸºç¡€ç›¸å…³æ€§åˆ†æ"""
        print("ğŸ”— æ‰§è¡ŒåŸºç¡€ç›¸å…³æ€§åˆ†æ...")

        correlations = {}

        # ä½ç½®ç‹¬ç«‹ç›¸å…³æ€§
        position_corrs = {}
        for pos in ['pos1', 'pos2', 'pos3']:
            pos_corrs = {}
            for metric in self.target_metrics:
                if metric in self.data.columns:
                    corr, p_value = pearsonr(self.features[pos], self.data[metric])
                    pos_corrs[metric] = {
                        'correlation': corr,
                        'p_value': p_value,
                        'significant': p_value < 0.05,
                        'effect_size': abs(corr)
                    }
            position_corrs[pos] = pos_corrs

        correlations['position_correlations'] = position_corrs

        # ç‰¹å¾é‡è¦æ€§
        feature_importance = {}
        for metric in self.target_metrics:
            if metric in self.data.columns:
                rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
                rf.fit(self.features, self.data[metric])
                importance_dict = dict(zip(self.features.columns, rf.feature_importances_))
                feature_importance[metric] = dict(sorted(importance_dict.items(),
                                                       key=lambda x: x[1], reverse=True))

        correlations['feature_importance'] = feature_importance
        self.results['correlations'] = correlations
        print("âœ… åŸºç¡€ç›¸å…³æ€§åˆ†æå®Œæˆ")
        return correlations

    def position_independent_analysis(self) -> Dict:
        """ä½ç½®ç‹¬ç«‹å½±å“åˆ†æ - æ§åˆ¶å…¶ä»–å˜é‡åˆ†æå•ä¸ªä½ç½®çš„çº¯å‡€æ•ˆåº”"""
        print("ğŸ¯ æ‰§è¡Œä½ç½®ç‹¬ç«‹å½±å“åˆ†æ...")

        independent_effects = {}

        for pos in ['pos1', 'pos2', 'pos3']:
            pos_effects = {}

            for metric in ['DifficultyScore', 'PeakDockCount', 'PressureValueMean']:
                if metric in self.data.columns:
                    # è®¡ç®—è¾¹é™…æ•ˆåº”ï¼šposæ¯å˜åŒ–1å•ä½å¯¹metricçš„å½±å“
                    marginal_effects = []

                    # åˆ†æä¸åŒæ•°å€¼åŒºé—´çš„æ•ˆåº”
                    for value in range(1, 10):
                        subset = self.data[self.features[pos] == value]
                        if len(subset) > 5:  # ç¡®ä¿æ ·æœ¬é‡è¶³å¤Ÿ
                            avg_metric = subset[metric].mean()
                            marginal_effects.append((value, avg_metric))

                    if len(marginal_effects) > 2:
                        # è®¡ç®—è¾¹é™…é€’å¢æ•ˆåº”
                        marginal_diffs = []
                        for i in range(1, len(marginal_effects)):
                            diff = marginal_effects[i][1] - marginal_effects[i-1][1]
                            marginal_diffs.append(diff)

                        pos_effects[metric] = {
                            'marginal_effects': marginal_effects,
                            'avg_marginal_diff': np.mean(marginal_diffs),
                            'marginal_volatility': np.std(marginal_diffs)
                        }

            independent_effects[pos] = pos_effects

        self.results['independent_effects'] = independent_effects
        print("âœ… ä½ç½®ç‹¬ç«‹å½±å“åˆ†æå®Œæˆ")
        return independent_effects

    def interaction_analysis(self) -> Dict:
        """ä½ç½®äº¤äº’æ•ˆåº”åˆ†æ - åˆ†æä½ç½®é—´ç›¸äº’ä½œç”¨"""
        print("ğŸ”„ æ‰§è¡Œä½ç½®äº¤äº’æ•ˆåº”åˆ†æ...")

        interaction_effects = {}

        # åŒä½ç½®äº¤äº’åˆ†æ
        for pos_pair in [('pos1', 'pos2'), ('pos1', 'pos3'), ('pos2', 'pos3')]:
            pair_key = f"{pos_pair[0]}Ã—{pos_pair[1]}"
            pair_effects = {}

            for metric in ['DifficultyScore', 'PeakDockCount']:
                if metric in self.data.columns:
                    # åˆ›å»ºäº¤äº’ç‰¹å¾
                    interaction_feature = self.features[pos_pair[0]] * self.features[pos_pair[1]]

                    # æ¯”è¾ƒæœ‰äº¤äº’é¡¹vsæ— äº¤äº’é¡¹çš„æ¨¡å‹æ€§èƒ½
                    X_base = self.features[[pos_pair[0], pos_pair[1]]]
                    X_inter = X_base.copy()
                    X_inter['interaction'] = interaction_feature

                    # åŸºç¡€æ¨¡å‹
                    rf_base = RandomForestRegressor(n_estimators=50, random_state=42)
                    rf_base.fit(X_base, self.data[metric])
                    r2_base = rf_base.score(X_base, self.data[metric])

                    # äº¤äº’æ¨¡å‹
                    rf_inter = RandomForestRegressor(n_estimators=50, random_state=42)
                    rf_inter.fit(X_inter, self.data[metric])
                    r2_inter = rf_inter.score(X_inter, self.data[metric])

                    pair_effects[metric] = {
                        'r2_base': r2_base,
                        'r2_interaction': r2_inter,
                        'interaction_gain': r2_inter - r2_base,
                        'interaction_strength': abs(r2_inter - r2_base)
                    }

            interaction_effects[pair_key] = pair_effects

        self.results['interaction_effects'] = interaction_effects
        print("âœ… ä½ç½®äº¤äº’æ•ˆåº”åˆ†æå®Œæˆ")
        return interaction_effects

    def dynamic_impact_analysis(self) -> Dict:
        """åŠ¨æ€å½±å“åˆ†æ - åˆ†æä½ç½®åœ¨ä¸åŒæ¸¸æˆé˜¶æ®µçš„å·®å¼‚åŒ–å½±å“"""
        print("â±ï¸ æ‰§è¡ŒåŠ¨æ€å½±å“åˆ†æ...")

        dynamic_effects = {}

        # è§£æDockAfterTrioMatchåºåˆ—æ•°æ®
        dock_sequences = []
        for dock_str in self.data['DockAfterTrioMatch'].fillna(''):
            if dock_str:
                try:
                    dock_values = [int(x) for x in str(dock_str).split(',')]
                    dock_sequences.append(dock_values)
                except:
                    dock_sequences.append([])
            else:
                dock_sequences.append([])

        # åˆ†æå„ä½ç½®å¯¹ä¸åŒæ¸¸æˆé˜¶æ®µçš„å½±å“
        for pos in ['pos1', 'pos2', 'pos3']:
            pos_dynamic = {}

            # åˆ†æå¯¹æ¸¸æˆå‰æœŸã€ä¸­æœŸã€åæœŸçš„å·®å¼‚åŒ–å½±å“
            stage_effects = {'early': [], 'middle': [], 'late': []}

            for i, dock_seq in enumerate(dock_sequences):
                if len(dock_seq) >= 6:  # ç¡®ä¿åºåˆ—è¶³å¤Ÿé•¿
                    seq_len = len(dock_seq)
                    early_avg = np.mean(dock_seq[:seq_len//3])
                    middle_avg = np.mean(dock_seq[seq_len//3:2*seq_len//3])
                    late_avg = np.mean(dock_seq[2*seq_len//3:])

                    pos_value = self.features.iloc[i][pos]
                    stage_effects['early'].append((pos_value, early_avg))
                    stage_effects['middle'].append((pos_value, middle_avg))
                    stage_effects['late'].append((pos_value, late_avg))

            # è®¡ç®—å„é˜¶æ®µçš„ç›¸å…³æ€§
            for stage, values in stage_effects.items():
                if len(values) > 10:
                    pos_vals, dock_vals = zip(*values)
                    corr, _ = pearsonr(pos_vals, dock_vals)
                    pos_dynamic[f'{stage}_correlation'] = corr

            dynamic_effects[pos] = pos_dynamic

        self.results['dynamic_effects'] = dynamic_effects
        print("âœ… åŠ¨æ€å½±å“åˆ†æå®Œæˆ")
        return dynamic_effects

    def mechanism_analysis(self) -> Dict:
        """å½±å“æœºåˆ¶åˆ†æ - åˆ†æä½ç½®å½±å“æŒ‡æ ‡çš„ä¸­ä»‹è·¯å¾„"""
        print("ğŸ” æ‰§è¡Œå½±å“æœºåˆ¶åˆ†æ...")

        mechanism_effects = {}

        # å¯¹äºæ¯ä¸ªä½ç½®ï¼Œåˆ†æå…¶å¯¹DifficultyScoreçš„ä¸­ä»‹è·¯å¾„
        for pos in ['pos1', 'pos2', 'pos3']:
            pos_mechanism = {}

            # ç›´æ¥æ•ˆåº”ï¼špos -> DifficultyScore
            if 'DifficultyScore' in self.data.columns:
                direct_corr, _ = pearsonr(self.features[pos], self.data['DifficultyScore'])
                pos_mechanism['direct_effect'] = direct_corr

                # ä¸­ä»‹æ•ˆåº”åˆ†æï¼špos -> ä¸­ä»‹å˜é‡ -> DifficultyScore
                mediators = ['PressureValueMean', 'PeakDockCount', 'PressureValueMax']
                mediation_effects = {}

                for mediator in mediators:
                    if mediator in self.data.columns:
                        # pos -> mediator
                        corr_pm, _ = pearsonr(self.features[pos], self.data[mediator])
                        # mediator -> DifficultyScore
                        corr_md, _ = pearsonr(self.data[mediator], self.data['DifficultyScore'])
                        # ä¸­ä»‹æ•ˆåº”å¼ºåº¦ = ä¸¤ä¸ªç›¸å…³ç³»æ•°çš„ä¹˜ç§¯
                        mediation_strength = corr_pm * corr_md

                        mediation_effects[mediator] = {
                            'pos_to_mediator': corr_pm,
                            'mediator_to_target': corr_md,
                            'mediation_strength': mediation_strength
                        }

                pos_mechanism['mediation_effects'] = mediation_effects

                # æ‰¾å‡ºæœ€å¼ºçš„ä¸­ä»‹è·¯å¾„
                if mediation_effects:
                    strongest_mediator = max(mediation_effects.items(),
                                           key=lambda x: abs(x[1]['mediation_strength']))
                    pos_mechanism['strongest_mediation_path'] = {
                        'mediator': strongest_mediator[0],
                        'strength': strongest_mediator[1]['mediation_strength']
                    }

            mechanism_effects[pos] = pos_mechanism

        self.results['mechanism_effects'] = mechanism_effects
        print("âœ… å½±å“æœºåˆ¶åˆ†æå®Œæˆ")
        return mechanism_effects

    def pattern_analysis(self) -> Dict:
        """é…ç½®æ¨¡å¼åˆ†æ"""
        print("ğŸ¯ æ‰§è¡Œé…ç½®æ¨¡å¼åˆ†æ...")

        patterns = {}

        # åºåˆ—æ¨¡å¼åˆ†æ
        sequence_patterns = {
            'increasing': self.data[self.features['is_increasing'] == 1],
            'decreasing': self.data[self.features['is_decreasing'] == 1],
            'uniform': self.data[self.features['is_uniform'] == 1],
            'mixed': self.data[(self.features['is_increasing'] == 0) &
                             (self.features['is_decreasing'] == 0) &
                             (self.features['is_uniform'] == 0)]
        }

        pattern_stats = {}
        for pattern_name, pattern_data in sequence_patterns.items():
            if len(pattern_data) > 0:
                stats_dict = {}
                for metric in self.target_metrics:
                    if metric in pattern_data.columns:
                        stats_dict[metric] = {
                            'mean': pattern_data[metric].mean(),
                            'std': pattern_data[metric].std(),
                            'count': len(pattern_data)
                        }
                pattern_stats[pattern_name] = stats_dict

        patterns['sequence_patterns'] = pattern_stats

        # æ•°å€¼åˆ†å¸ƒåˆ†æ
        value_analysis = {}
        for pos in ['pos1', 'pos2', 'pos3']:
            pos_analysis = {}
            for value in range(1, 10):
                value_data = self.data[self.features[pos] == value]
                if len(value_data) > 0:
                    value_stats = {}
                    for metric in self.target_metrics:
                        if metric in value_data.columns:
                            value_stats[metric] = {
                                'mean': value_data[metric].mean(),
                                'count': len(value_data)
                            }
                    pos_analysis[f'value_{value}'] = value_stats
            value_analysis[pos] = pos_analysis

        patterns['value_distribution'] = value_analysis

        self.results['patterns'] = patterns
        print("âœ… é…ç½®æ¨¡å¼åˆ†æå®Œæˆ")
        return patterns

    def build_prediction_models(self) -> Dict:
        """æ„å»ºé¢„æµ‹æ¨¡å‹"""
        print("ğŸ¤– æ„å»ºé¢„æµ‹æ¨¡å‹...")

        models = {}

        for metric in self.target_metrics:
            if metric not in self.data.columns:
                continue

            # æ•°æ®å‡†å¤‡
            X = self.features
            y = self.data[metric]

            # è®­ç»ƒæµ‹è¯•åˆ†å‰²
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42
            )

            # æ•°æ®æ ‡å‡†åŒ–
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)

            # éšæœºæ£®æ—æ¨¡å‹
            rf_model = RandomForestRegressor(
                n_estimators=100,
                random_state=42,
                n_jobs=-1
            )

            rf_model.fit(X_train_scaled, y_train)

            # æ¨¡å‹è¯„ä¼°
            y_pred = rf_model.predict(X_test_scaled)
            r2 = r2_score(y_test, y_pred)
            rmse = np.sqrt(mean_squared_error(y_test, y_pred))

            # ç‰¹å¾é‡è¦æ€§
            feature_importance = dict(zip(X.columns, rf_model.feature_importances_))

            models[metric] = {
                'model': rf_model,
                'scaler': scaler,
                'r2_score': r2,
                'rmse': rmse,
                'feature_importance': feature_importance
            }

        self.results['models'] = models
        print(f"âœ… é¢„æµ‹æ¨¡å‹æ„å»ºå®Œæˆï¼Œå…±{len(models)}ä¸ªæ¨¡å‹")
        return models

    def run_complete_analysis(self, output_dir: str = None) -> Dict:
        """è¿è¡Œå‡çº§ç‰ˆå®Œæ•´åˆ†ææµç¨‹"""
        print("ğŸš€ å¼€å§‹æ·±åº¦ä½“éªŒé…ç½®å½±å“åˆ†æ...")

        # æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
        self.load_and_preprocess()

        # åŸºç¡€åˆ†æ
        correlations = self.correlation_analysis()
        patterns = self.pattern_analysis()

        # æ·±åº¦åˆ†æï¼ˆæ–°å¢ï¼‰
        independent_effects = self.position_independent_analysis()
        interaction_effects = self.interaction_analysis()
        dynamic_effects = self.dynamic_impact_analysis()
        mechanism_effects = self.mechanism_analysis()

        # æœºå™¨å­¦ä¹ å»ºæ¨¡
        models = self.build_prediction_models()

        # åˆ›å»ºå¢å¼ºå¯è§†åŒ–
        self.create_enhanced_visualizations(output_dir)

        # ç”Ÿæˆæ·±åº¦æŠ¥å‘Š
        report_path = self.generate_enhanced_report()

        # è¾“å‡ºå…³é”®å‘ç°
        self._print_key_findings()

        # æ±‡æ€»ç»“æœ
        summary = {
            'data_summary': {
                'total_records': len(self.data),
                'unique_configs': len(self.data[['pos1', 'pos2', 'pos3']].drop_duplicates()),
                'target_metrics': len(self.target_metrics)
            },
            'analysis_results': {
                'basic_analysis': len(correlations) > 0 and len(patterns) > 0,
                'advanced_analysis': len(independent_effects) > 0 and len(interaction_effects) > 0,
                'dynamic_analysis': len(dynamic_effects) > 0,
                'mechanism_analysis': len(mechanism_effects) > 0,
                'models_completed': len(models) > 0,
                'best_model_r2': max([m['r2_score'] for m in models.values()]) if models else 0
            },
            'output_files': {
                'report_path': report_path,
                'charts_directory': output_dir or (
                    Path(self.csv_path).parent / "analysis_charts" if self.csv_path
                    else Path(self.csv_directory) / "analysis_output"
                )
            }
        }

        print("ğŸ‰ æ·±åº¦åˆ†ææµç¨‹æ‰§è¡Œå®Œæˆ!")
        print(f"ğŸ“„ æŠ¥å‘Šè·¯å¾„: {summary['output_files']['report_path']}")
        print(f"ğŸ“Š å›¾è¡¨ç›®å½•: {summary['output_files']['charts_directory']}")

        return summary

    def create_enhanced_visualizations(self, output_dir: str = None):
        """åˆ›å»ºå¢å¼ºå¯è§†åŒ–å›¾è¡¨"""
        print("ğŸ“Š åˆ›å»ºå¢å¼ºå¯è§†åŒ–å›¾è¡¨...")

        if output_dir is None:
            # å¤šæ–‡ä»¶æ¨¡å¼ä½¿ç”¨csv_directoryï¼Œå•æ–‡ä»¶æ¨¡å¼ä½¿ç”¨csv_path
            if self.csv_path:
                output_dir = Path(self.csv_path).parent / "analysis_charts"
            else:
                output_dir = Path(self.csv_directory) / "analysis_output"

        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)

        # å®‰å…¨å›¾è¡¨ç»˜åˆ¶åˆ—è¡¨
        charts = [
            ("ä½ç½®ç›¸å…³æ€§çƒ­åŠ›å›¾", self._plot_position_correlation_heatmap),
            ("é…ç½®æ¨¡å¼ç®±çº¿å›¾", self._plot_pattern_boxplots),
            ("ç‰¹å¾é‡è¦æ€§å›¾", self._plot_feature_importance),
            ("é…ç½®åˆ†å¸ƒå›¾", self._plot_config_distribution),
            ("æ¨¡å‹æ€§èƒ½å›¾", self._plot_model_performance),
            ("ä½ç½®ç‹¬ç«‹æ•ˆåº”å›¾", self._plot_independent_effects),
            ("äº¤äº’æ•ˆåº”å›¾", self._plot_interaction_effects),
            ("æœºåˆ¶åˆ†æå›¾", self._plot_mechanism_analysis)
        ]

        successful_charts = 0
        for chart_name, chart_func in charts:
            try:
                print(f"   ç»˜åˆ¶ {chart_name}...")
                chart_func(output_path)
                successful_charts += 1
                print(f"   âœ… {chart_name} å®Œæˆ")
            except Exception as e:
                print(f"   âŒ {chart_name} å¤±è´¥: {str(e)}")
                continue

        print(f"âœ… å¢å¼ºå¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {output_path}")
        print(f"ğŸ“Š æˆåŠŸç»˜åˆ¶: {successful_charts}/{len(charts)} ä¸ªå›¾è¡¨")

    def _plot_independent_effects(self, output_path: Path):
        """ç»˜åˆ¶ä½ç½®ç‹¬ç«‹æ•ˆåº”å›¾"""
        if 'independent_effects' not in self.results:
            return

        fig, axes = plt.subplots(1, 3, figsize=(18, 6))

        for i, pos in enumerate(['pos1', 'pos2', 'pos3']):
            if pos in self.results['independent_effects']:
                effects = self.results['independent_effects'][pos]

                if 'DifficultyScore' in effects:
                    marginal_data = effects['DifficultyScore']['marginal_effects']
                    if marginal_data:
                        x_vals, y_vals = zip(*marginal_data)
                        axes[i].plot(x_vals, y_vals, 'o-', linewidth=2, markersize=8)
                        axes[i].set_title(f'{pos}å¯¹DifficultyScoreçš„è¾¹é™…æ•ˆåº”')
                        axes[i].set_xlabel(f'{pos}æ•°å€¼')
                        axes[i].set_ylabel('DifficultyScoreå‡å€¼')
                        axes[i].grid(True, alpha=0.3)

        plt.suptitle('ä½ç½®ç‹¬ç«‹æ•ˆåº”åˆ†æ', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(output_path / 'independent_effects.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_interaction_effects(self, output_path: Path):
        """ç»˜åˆ¶äº¤äº’æ•ˆåº”å¼ºåº¦å›¾ï¼Œå¢åŠ æ•°å€¼èŒƒå›´æ£€æŸ¥"""
        if 'interaction_effects' not in self.results:
            return

        pairs = list(self.results['interaction_effects'].keys())
        metrics = ['DifficultyScore', 'PeakDockCount']

        if not pairs:
            print("   âš ï¸ æ— äº¤äº’æ•ˆåº”æ•°æ®ï¼Œè·³è¿‡ç»˜åˆ¶")
            return

        # æ”¶é›†æ‰€æœ‰æ•°å€¼å¹¶æ£€æŸ¥èŒƒå›´
        all_strengths = []
        for pair in pairs:
            for metric in metrics:
                if metric in self.results['interaction_effects'][pair]:
                    strength = self.results['interaction_effects'][pair][metric]['interaction_strength']
                    if np.isfinite(strength):  # æ£€æŸ¥æ•°å€¼æœ‰æ•ˆæ€§
                        all_strengths.append(abs(strength))

        if not all_strengths:
            print("   âš ï¸ æ— æœ‰æ•ˆäº¤äº’æ•ˆåº”å¼ºåº¦æ•°æ®ï¼Œè·³è¿‡ç»˜åˆ¶")
            return

        # æ£€æŸ¥æ•°å€¼èŒƒå›´åˆç†æ€§
        max_strength = max(all_strengths)
        if max_strength > 1e6:  # å¼‚å¸¸å¤§æ•°å€¼
            print(f"   âš ï¸ æ£€æµ‹åˆ°å¼‚å¸¸å¤§æ•°å€¼({max_strength:.2e})ï¼Œè·³è¿‡äº¤äº’æ•ˆåº”å›¾ç»˜åˆ¶")
            return

        try:
            fig, ax = plt.subplots(figsize=(min(12, len(pairs) * 2), 6))

            x_pos = np.arange(len(pairs))
            width = 0.35

            for i, metric in enumerate(metrics):
                strengths = []
                for pair in pairs:
                    if metric in self.results['interaction_effects'][pair]:
                        strength = self.results['interaction_effects'][pair][metric]['interaction_strength']
                        # æ•°å€¼å®‰å…¨æ£€æŸ¥
                        if np.isfinite(strength):
                            strengths.append(max(0, min(1, abs(strength))))  # é™åˆ¶åœ¨[0,1]èŒƒå›´
                        else:
                            strengths.append(0)
                    else:
                        strengths.append(0)

                if any(s > 0 for s in strengths):  # åªç»˜åˆ¶æœ‰æ•°æ®çš„æŒ‡æ ‡
                    bars = ax.bar(x_pos + i * width, strengths, width, label=metric, alpha=0.7)

                    # æ·»åŠ æ•°å€¼æ ‡ç­¾
                    for bar, strength in zip(bars, strengths):
                        if strength > 0.001:  # åªæ˜¾ç¤ºæ˜¾è‘—å€¼
                            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max_strength*0.01,
                                   f'{strength:.3f}', ha='center', va='bottom', fontsize=9)

            ax.set_xlabel('ä½ç½®ç»„åˆ')
            ax.set_ylabel('äº¤äº’æ•ˆåº”å¼ºåº¦')
            ax.set_title('ä½ç½®é—´äº¤äº’æ•ˆåº”åˆ†æ')
            ax.set_xticks(x_pos + width / 2)
            ax.set_xticklabels(pairs, rotation=45 if len(pairs) > 3 else 0)
            ax.legend()
            ax.grid(True, alpha=0.3)

            # è®¾ç½®åˆç†çš„Yè½´èŒƒå›´
            ax.set_ylim(0, max(all_strengths) * 1.1)

            plt.tight_layout()
            plt.savefig(output_path / 'interaction_effects.png', dpi=300, bbox_inches='tight')
            plt.close()

        except Exception as e:
            print(f"   âŒ äº¤äº’æ•ˆåº”å›¾ç»˜åˆ¶å†…éƒ¨é”™è¯¯: {str(e)}")
            plt.close('all')  # ç¡®ä¿æ¸…ç†èµ„æº

    def _plot_mechanism_analysis(self, output_path: Path):
        """ç»˜åˆ¶æœºåˆ¶åˆ†æå›¾"""
        if 'mechanism_effects' not in self.results:
            return

        fig, axes = plt.subplots(1, 3, figsize=(18, 6))

        for i, pos in enumerate(['pos1', 'pos2', 'pos3']):
            if pos in self.results['mechanism_effects']:
                mechanism = self.results['mechanism_effects'][pos]

                if 'mediation_effects' in mechanism:
                    mediators = list(mechanism['mediation_effects'].keys())
                    strengths = [mechanism['mediation_effects'][m]['mediation_strength']
                               for m in mediators]

                    colors = ['red' if s > 0 else 'blue' for s in strengths]
                    bars = axes[i].barh(mediators, [abs(s) for s in strengths], color=colors, alpha=0.7)

                    axes[i].set_title(f'{pos}çš„ä¸­ä»‹æ•ˆåº”åˆ†æ')
                    axes[i].set_xlabel('ä¸­ä»‹æ•ˆåº”å¼ºåº¦')

                    # æ·»åŠ æ•°å€¼æ ‡ç­¾
                    for bar, strength in zip(bars, strengths):
                        axes[i].text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,
                                   f'{strength:.3f}', va='center', fontsize=9)

        plt.suptitle('å½±å“æœºåˆ¶ä¸­ä»‹è·¯å¾„åˆ†æ', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(output_path / 'mechanism_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()

    def generate_enhanced_report(self, output_path: str = None) -> str:
        """ç”Ÿæˆå¢å¼ºç‰ˆåˆ†ææŠ¥å‘Š"""
        print("ğŸ“‹ ç”Ÿæˆæ·±åº¦åˆ†ææŠ¥å‘Š...")

        if output_path is None:
            # å¤šæ–‡ä»¶æ¨¡å¼ä½¿ç”¨csv_directoryï¼Œå•æ–‡ä»¶æ¨¡å¼ä½¿ç”¨csv_path
            if self.csv_path:
                output_path = Path(self.csv_path).parent / "enhanced_analysis_report.md"
            else:
                output_path = Path(self.csv_directory) / "enhanced_analysis_report.md"

        report = []
        report.append("# ä½“éªŒæ¨¡å¼é…ç½®[x,y,z]æ·±åº¦å½±å“åˆ†ææŠ¥å‘Š\n")

        # æ•°æ®æºä¿¡æ¯
        if self.csv_path:
            report.append(f"**æ•°æ®æº**: {self.csv_path}")
        else:
            report.append(f"**æ•°æ®æº**: {self.csv_directory} ({len(self.csv_files)}ä¸ªCSVæ–‡ä»¶)")

        report.append(f"**åˆ†ææ—¶é—´**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"**æ ·æœ¬æ•°é‡**: {len(self.data):,}æ¡\n")

        # æ•°æ®æ¦‚è§ˆ
        self._add_data_overview(report)

        # ä½ç½®ç‹¬ç«‹æ•ˆåº”åˆ†æ
        self._add_independent_effects_report(report)

        # äº¤äº’æ•ˆåº”åˆ†æ
        self._add_interaction_effects_report(report)

        # åŠ¨æ€å½±å“åˆ†æ
        self._add_dynamic_effects_report(report)

        # æœºåˆ¶åˆ†æ
        self._add_mechanism_effects_report(report)

        # å…³é”®å‘ç°ä¸å»ºè®®
        self._add_key_findings_and_recommendations(report)

        # å†™å…¥æ–‡ä»¶
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report))

        print(f"âœ… æ·±åº¦åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
        return str(output_path)
    

    def _add_data_overview(self, report):
        report.append("## ğŸ“Š æ•°æ®æ¦‚è§ˆ\n")
        report.append(f"- é…ç½®èŒƒå›´: pos1=[{self.features['pos1'].min()}-{self.features['pos1'].max()}], " +
                     f"pos2=[{self.features['pos2'].min()}-{self.features['pos2'].max()}], " +
                     f"pos3=[{self.features['pos3'].min()}-{self.features['pos3'].max()}]")
        report.append(f"- å”¯ä¸€é…ç½®æ•°: {len(self.data[['pos1', 'pos2', 'pos3']].drop_duplicates())}ç§")

    def _add_independent_effects_report(self, report):
        if 'independent_effects' not in self.results:
            return

        report.append("\n## ğŸ¯ ä½ç½®ç‹¬ç«‹æ•ˆåº”åˆ†æ\n")

        for pos in ['pos1', 'pos2', 'pos3']:
            if pos in self.results['independent_effects']:
                effects = self.results['independent_effects'][pos]
                report.append(f"### {pos}çš„ç‹¬ç«‹æ•ˆåº”:")

                for metric, data in effects.items():
                    if 'avg_marginal_diff' in data:
                        avg_diff = data['avg_marginal_diff']
                        volatility = data['marginal_volatility']
                        report.append(f"- **{metric}**: å¹³å‡è¾¹é™…æ•ˆåº” {avg_diff:.3f}, æ³¢åŠ¨æ€§ {volatility:.3f}")

    def _add_interaction_effects_report(self, report):
        if 'interaction_effects' not in self.results:
            return

        report.append("\n## ğŸ”„ ä½ç½®äº¤äº’æ•ˆåº”åˆ†æ\n")

        for pair, effects in self.results['interaction_effects'].items():
            report.append(f"### {pair}äº¤äº’æ•ˆåº”:")
            for metric, data in effects.items():
                gain = data['interaction_gain']
                strength = data['interaction_strength']
                report.append(f"- **{metric}**: äº¤äº’å¢ç›Š {gain:.4f}, æ•ˆåº”å¼ºåº¦ {strength:.4f}")

    def _add_dynamic_effects_report(self, report):
        if 'dynamic_effects' not in self.results:
            return

        report.append("\n## â±ï¸ åŠ¨æ€å½±å“åˆ†æ\n")

        for pos, dynamics in self.results['dynamic_effects'].items():
            report.append(f"### {pos}çš„æ—¶åºå½±å“:")
            for stage, corr in dynamics.items():
                stage_name = {'early_correlation': 'å‰æœŸ', 'middle_correlation': 'ä¸­æœŸ', 'late_correlation': 'åæœŸ'}.get(stage, stage)
                report.append(f"- **{stage_name}**: ç›¸å…³æ€§ {corr:.3f}")

    def _add_mechanism_effects_report(self, report):
        if 'mechanism_effects' not in self.results:
            return

        report.append("\n## ğŸ” å½±å“æœºåˆ¶åˆ†æ\n")

        for pos, mechanism in self.results['mechanism_effects'].items():
            report.append(f"### {pos}çš„å½±å“æœºåˆ¶:")
            if 'direct_effect' in mechanism:
                report.append(f"- **ç›´æ¥æ•ˆåº”**: {mechanism['direct_effect']:.3f}")

            if 'strongest_mediation_path' in mechanism:
                strongest = mechanism['strongest_mediation_path']
                report.append(f"- **æœ€å¼ºä¸­ä»‹è·¯å¾„**: {strongest['mediator']} (strength: {strongest['strength']:.3f})")

    def _add_key_findings_and_recommendations(self, report):
        report.append("\n## ğŸ’¡ å…³é”®å‘ç°ä¸å»ºè®®\n")

        # ä½ç½®é‡è¦æ€§æ’åº
        if 'correlations' in self.results:
            pos_importance = {}
            for pos in ['pos1', 'pos2', 'pos3']:
                avg_abs_corr = np.mean([
                    abs(self.results['correlations']['position_correlations'][pos][m]['correlation'])
                    for m in ['DifficultyScore', 'PeakDockCount', 'PressureValueMean']
                    if m in self.results['correlations']['position_correlations'][pos]
                ])
                pos_importance[pos] = avg_abs_corr

            sorted_positions = sorted(pos_importance.items(), key=lambda x: x[1], reverse=True)
            report.append("### ä¸»è¦å‘ç°:")
            report.append(f"1. **ä½ç½®é‡è¦æ€§æ’åº**: {' > '.join([f'{p}({v:.3f})' for p, v in sorted_positions])}")

        # å…³é”®äº¤äº’æ•ˆåº”
        if 'interaction_effects' in self.results:
            max_interaction = None
            max_strength = 0
            for pair, effects in self.results['interaction_effects'].items():
                for metric, data in effects.items():
                    if data['interaction_strength'] > max_strength:
                        max_strength = data['interaction_strength']
                        max_interaction = f"{pair}->{metric}"
            if max_interaction:
                report.append(f"2. **æœ€å…³é”®äº¤äº’æ•ˆåº”**: {max_interaction} (strength: {max_strength:.3f})")

        report.append("\n### ä¼˜åŒ–å»ºè®®:")
        report.append("1. é‡ç‚¹å…³æ³¨å½±å“åŠ›æœ€å¤§çš„ä½ç½®å‚æ•°")
        report.append("2. è€ƒè™‘ä½ç½®é—´çš„äº¤äº’æ•ˆåº”ï¼Œé¿å…å•çº¯çš„ç‹¬ç«‹è°ƒæ•´")
        report.append("3. æ ¹æ®ä¸­ä»‹æœºåˆ¶é’ˆå¯¹æ€§ä¼˜åŒ–ï¼Œæé«˜è°ƒæ•´ç²¾åº¦")

    def _print_key_findings(self):
        """è¾“å‡ºå…³é”®å‘ç°æ‘˜è¦"""
        print("\nğŸ’¡ === å…³é”®å‘ç°æ‘˜è¦ ===")

        # ä½ç½®é‡è¦æ€§æ’åº
        if 'correlations' in self.results:
            pos_importance = {}
            for pos in ['pos1', 'pos2', 'pos3']:
                avg_abs_corr = np.mean([
                    abs(self.results['correlations']['position_correlations'][pos][m]['correlation'])
                    for m in ['DifficultyScore', 'PeakDockCount', 'PressureValueMean']
                    if m in self.results['correlations']['position_correlations'][pos]
                ])
                pos_importance[pos] = avg_abs_corr

            sorted_positions = sorted(pos_importance.items(), key=lambda x: x[1], reverse=True)
            print(f"ğŸ¯ ä½ç½®é‡è¦æ€§æ’åº: {' > '.join([f'{p}({v:.3f})' for p, v in sorted_positions])}")

        # æœ€å¼ºäº¤äº’æ•ˆåº”
        if 'interaction_effects' in self.results:
            max_interaction = None
            max_strength = 0
            for pair, effects in self.results['interaction_effects'].items():
                for metric, data in effects.items():
                    if data['interaction_strength'] > max_strength:
                        max_strength = data['interaction_strength']
                        max_interaction = f"{pair}->{metric}"
            if max_interaction:
                print(f"ğŸ”„ æœ€å¼ºäº¤äº’æ•ˆåº”: {max_interaction} (strength: {max_strength:.3f})")

        # æœ€å…³é”®ä¸­ä»‹è·¯å¾„
        if 'mechanism_effects' in self.results:
            strongest_mediation = None
            max_mediation_strength = 0
            for pos, mechanism in self.results['mechanism_effects'].items():
                if 'strongest_mediation_path' in mechanism:
                    strength = abs(mechanism['strongest_mediation_path']['strength'])
                    if strength > max_mediation_strength:
                        max_mediation_strength = strength
                        strongest_mediation = f"{pos}->{mechanism['strongest_mediation_path']['mediator']}->DifficultyScore"
            if strongest_mediation:
                print(f"ğŸ” æœ€å…³é”®ä¸­ä»‹è·¯å¾„: {strongest_mediation} (strength: {max_mediation_strength:.3f})")

    # åŸºç¡€å¯è§†åŒ–æ–¹æ³•ï¼ˆç®€åŒ–ç‰ˆå®ç°ï¼‰
    def _plot_position_correlation_heatmap(self, output_path: Path):
        """ç»˜åˆ¶ä½ç½®ç›¸å…³æ€§çƒ­åŠ›å›¾"""
        if 'correlations' not in self.results:
            return

        fig, ax = plt.subplots(figsize=(10, 8))

        # æ„å»ºç›¸å…³æ€§çŸ©é˜µ
        positions = ['pos1', 'pos2', 'pos3']
        metrics = ['DifficultyScore', 'PeakDockCount', 'PressureValueMean']
        corr_matrix = []

        for pos in positions:
            pos_corrs = []
            for metric in metrics:
                if pos in self.results['correlations']['position_correlations'] and \
                   metric in self.results['correlations']['position_correlations'][pos]:
                    corr = self.results['correlations']['position_correlations'][pos][metric]['correlation']
                    pos_corrs.append(corr)
                else:
                    pos_corrs.append(0)
            corr_matrix.append(pos_corrs)

        corr_matrix = np.array(corr_matrix)

        # ç»˜åˆ¶çƒ­åŠ›å›¾
        im = ax.imshow(corr_matrix, cmap='RdBu_r', vmin=-1, vmax=1, aspect='auto')

        # è®¾ç½®æ ‡ç­¾
        ax.set_xticks(range(len(metrics)))
        ax.set_xticklabels(metrics, rotation=45, ha='right')
        ax.set_yticks(range(len(positions)))
        ax.set_yticklabels(positions)

        # æ·»åŠ é¢œè‰²æ¡
        plt.colorbar(im, ax=ax, label='ç›¸å…³ç³»æ•°')

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i in range(len(positions)):
            for j in range(len(metrics)):
                ax.text(j, i, f'{corr_matrix[i, j]:.3f}',
                       ha='center', va='center', color='white' if abs(corr_matrix[i, j]) > 0.5 else 'black')

        plt.title('ä½ç½®-æŒ‡æ ‡ç›¸å…³æ€§çƒ­åŠ›å›¾')
        plt.tight_layout()
        plt.savefig(output_path / 'position_correlation_heatmap.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_pattern_boxplots(self, output_path: Path):
        """ç»˜åˆ¶é…ç½®æ¨¡å¼ç®±çº¿å›¾"""
        if 'patterns' not in self.results:
            return

        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        patterns = self.results['patterns']['sequence_patterns']

        metrics = ['DifficultyScore', 'PeakDockCount', 'PressureValueMean', 'TotalMoves']

        for idx, metric in enumerate(metrics):
            ax = axes[idx//2, idx%2]
            pattern_names = []
            pattern_means = []
            pattern_stds = []

            for pattern_name, pattern_data in patterns.items():
                if metric in pattern_data and 'mean' in pattern_data[metric]:
                    pattern_names.append(pattern_name)
                    pattern_means.append(pattern_data[metric]['mean'])
                    pattern_stds.append(pattern_data[metric].get('std', 0))

            if pattern_names:
                bars = ax.bar(pattern_names, pattern_means, yerr=pattern_stds, alpha=0.7, capsize=5)
                ax.set_title(f'{metric}çš„é…ç½®æ¨¡å¼å¯¹æ¯”')
                ax.set_ylabel(metric)
                ax.tick_params(axis='x', rotation=45)

                # æ·»åŠ æ•°å€¼æ ‡ç­¾
                for bar, mean in zip(bars, pattern_means):
                    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + bar.get_height()*0.01,
                           f'{mean:.2f}', ha='center', va='bottom')

        plt.suptitle('é…ç½®æ¨¡å¼åˆ†æç®±çº¿å›¾')
        plt.tight_layout()
        plt.savefig(output_path / 'pattern_boxplots.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_feature_importance(self, output_path: Path):
        """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾"""
        if 'models' not in self.results:
            return

        key_metrics = ['DifficultyScore', 'PeakDockCount', 'PressureValueMean']
        fig, axes = plt.subplots(1, len(key_metrics), figsize=(18, 6))

        for i, metric in enumerate(key_metrics):
            if metric in self.results['models']:
                importance = self.results['models'][metric]['feature_importance']

                # å–å‰8ä¸ªé‡è¦ç‰¹å¾
                top_features = dict(sorted(importance.items(), key=lambda x: x[1], reverse=True)[:8])

                features = list(top_features.keys())
                values = list(top_features.values())

                bars = axes[i].barh(features, values, alpha=0.7)
                axes[i].set_title(f'{metric}\nç‰¹å¾é‡è¦æ€§ (RÂ²={self.results["models"][metric]["r2_score"]:.3f})')
                axes[i].set_xlabel('é‡è¦æ€§åˆ†æ•°')

                # é¢œè‰²æ˜ å°„
                colors = plt.cm.viridis(np.linspace(0, 1, len(values)))
                for bar, color in zip(bars, colors):
                    bar.set_color(color)

        plt.suptitle('æœºå™¨å­¦ä¹ æ¨¡å‹ç‰¹å¾é‡è¦æ€§åˆ†æ')
        plt.tight_layout()
        plt.savefig(output_path / 'feature_importance.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_config_distribution(self, output_path: Path):
        """ç»˜åˆ¶é…ç½®åˆ†å¸ƒæ•£ç‚¹å›¾ï¼Œå¢åŠ æ•°å€¼å®‰å…¨æ£€æŸ¥"""
        try:
            # æ£€æŸ¥å¿…è¦æ•°æ®
            if len(self.features) == 0 or len(self.data) == 0:
                print("   âš ï¸ æ— é…ç½®åˆ†å¸ƒæ•°æ®ï¼Œè·³è¿‡ç»˜åˆ¶")
                return

            fig, axes = plt.subplots(2, 2, figsize=(15, 12))

            # 3Dé…ç½®ç©ºé—´æŠ•å½± (pos1 vs pos2ï¼Œé¢œè‰²è¡¨ç¤ºpos3)
            pos1_vals = self.features['pos1'].values
            pos2_vals = self.features['pos2'].values
            pos3_vals = self.features['pos3'].values

            # æ•°å€¼èŒƒå›´æ£€æŸ¥
            if np.any(~np.isfinite(pos1_vals)) or np.any(~np.isfinite(pos2_vals)) or np.any(~np.isfinite(pos3_vals)):
                print("   âš ï¸ é…ç½®æ•°æ®åŒ…å«æ— æ•ˆå€¼ï¼Œä½¿ç”¨æœ‰æ•ˆå­é›†")
                valid_mask = np.isfinite(pos1_vals) & np.isfinite(pos2_vals) & np.isfinite(pos3_vals)
                pos1_vals = pos1_vals[valid_mask]
                pos2_vals = pos2_vals[valid_mask]
                pos3_vals = pos3_vals[valid_mask]

            if len(pos1_vals) > 0:
                scatter = axes[0,0].scatter(pos1_vals, pos2_vals, c=pos3_vals,
                                          cmap='viridis', alpha=0.6, s=1)
                axes[0,0].set_xlabel('ä½ç½®1æ•°å€¼')
                axes[0,0].set_ylabel('ä½ç½®2æ•°å€¼')
                axes[0,0].set_title('é…ç½®ç©ºé—´åˆ†å¸ƒ (é¢œè‰²=pos3)')
                plt.colorbar(scatter, ax=axes[0,0], label='pos3æ•°å€¼')

            # é…ç½®ä¸DifficultyScoreçš„å…³ç³»
            if 'DifficultyScore' in self.data.columns:
                config_sum = self.features['config_sum'].values
                difficulty = self.data['DifficultyScore'].values

                # è¿‡æ»¤æ— æ•ˆå€¼
                valid_mask = np.isfinite(config_sum) & np.isfinite(difficulty)
                if np.any(valid_mask):
                    axes[0,1].scatter(config_sum[valid_mask], difficulty[valid_mask], alpha=0.3, s=1)
                    axes[0,1].set_xlabel('é…ç½®æ€»å’Œ')
                    axes[0,1].set_ylabel('DifficultyScore')
                    axes[0,1].set_title('é…ç½®æ€»å’Œ vs éš¾åº¦åˆ†æ•°')

            # é…ç½®æ ‡å‡†å·®åˆ†å¸ƒ
            config_std = self.features['config_std'].values
            valid_std = config_std[np.isfinite(config_std)]
            if len(valid_std) > 0:
                axes[1,0].hist(valid_std, bins=min(30, len(valid_std)//10), alpha=0.7, edgecolor='black')
                axes[1,0].set_xlabel('é…ç½®æ ‡å‡†å·®')
                axes[1,0].set_ylabel('é¢‘æ¬¡')
                axes[1,0].set_title('é…ç½®æ ‡å‡†å·®åˆ†å¸ƒ')

            # æå€¼é…ç½®æ•ˆåº”åˆ†æ
            if 'DifficultyScore' in self.data.columns:
                extreme_low = self.data[self.features['has_extreme_low'] == 1]
                extreme_high = self.data[self.features['has_extreme_high'] == 1]
                normal = self.data[(self.features['has_extreme_low'] == 0) & (self.features['has_extreme_high'] == 0)]

                if len(extreme_low) > 0 and len(extreme_high) > 0 and len(normal) > 0:
                    means = []
                    categories = []

                    for name, group in [('æä½é…ç½®', extreme_low), ('æ­£å¸¸é…ç½®', normal), ('æé«˜é…ç½®', extreme_high)]:
                        difficulty_vals = group['DifficultyScore'].values
                        valid_vals = difficulty_vals[np.isfinite(difficulty_vals)]
                        if len(valid_vals) > 0:
                            categories.append(name)
                            means.append(np.mean(valid_vals))

                    if means:
                        colors = ['red', 'gray', 'blue'][:len(means)]
                        bars = axes[1,1].bar(categories, means, alpha=0.7, color=colors)
                        axes[1,1].set_ylabel('å¹³å‡DifficultyScore')
                        axes[1,1].set_title('æå€¼é…ç½®æ•ˆåº”åˆ†æ')

                        # æ·»åŠ æ•°å€¼æ ‡ç­¾
                        for bar, mean in zip(bars, means):
                            if np.isfinite(mean):
                                axes[1,1].text(bar.get_x() + bar.get_width()/2,
                                              bar.get_height() + bar.get_height()*0.01,
                                              f'{mean:.2f}', ha='center', va='bottom')

            plt.suptitle('ä½“éªŒé…ç½®åˆ†å¸ƒç‰¹å¾åˆ†æ')
            plt.tight_layout()
            plt.savefig(output_path / 'config_distribution.png', dpi=300, bbox_inches='tight')
            plt.close()

        except Exception as e:
            print(f"   âŒ é…ç½®åˆ†å¸ƒå›¾ç»˜åˆ¶é”™è¯¯: {str(e)}")
            plt.close('all')

    def _plot_model_performance(self, output_path: Path):
        """ç»˜åˆ¶æ¨¡å‹æ€§èƒ½å›¾"""
        if 'models' not in self.results:
            return

        metrics = list(self.results['models'].keys())
        r2_scores = [self.results['models'][m]['r2_score'] for m in metrics]
        rmse_scores = [self.results['models'][m]['rmse'] for m in metrics]

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

        # RÂ²åˆ†æ•°
        bars1 = ax1.bar(metrics, r2_scores, color='skyblue', alpha=0.7)
        ax1.set_title('é¢„æµ‹æ¨¡å‹RÂ²æ€§èƒ½è¯„åˆ†')
        ax1.set_ylabel('RÂ²åˆ†æ•°')
        ax1.tick_params(axis='x', rotation=45)
        ax1.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='åŸºå‡†çº¿(0.5)')
        ax1.legend()

        # ä¸ºæ¯ä¸ªbaræ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, score in zip(bars1, r2_scores):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{score:.3f}', ha='center', va='bottom')

        # RMSEåˆ†æ•°
        bars2 = ax2.bar(metrics, rmse_scores, color='lightcoral', alpha=0.7)
        ax2.set_title('é¢„æµ‹æ¨¡å‹RMSEè¯¯å·®')
        ax2.set_ylabel('RMSE')
        ax2.tick_params(axis='x', rotation=45)

        plt.suptitle('æœºå™¨å­¦ä¹ æ¨¡å‹æ€§èƒ½è¯„ä¼°')
        plt.tight_layout()
        plt.savefig(output_path / 'model_performance.png', dpi=300, bbox_inches='tight')
        plt.close()

    def create_visualizations(self, output_dir: str = None):
        """å…¼å®¹æ–¹æ³•ï¼šåˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        self.create_enhanced_visualizations(output_dir)

    def generate_report(self, output_path: str = None) -> str:
        """å…¼å®¹æ–¹æ³•ï¼šç”ŸæˆåŸºç¡€åˆ†ææŠ¥å‘Š"""
        return self.generate_enhanced_report(output_path)


def main():
    """ä¸»å‡½æ•° - è¿è¡Œæ·±åº¦åˆ†æ"""
    import sys
    import os

    # è®¾ç½®UTF-8ç¼–ç è¾“å‡º
    if sys.platform.startswith('win'):
        os.system('chcp 65001 >nul 2>&1')

    print("ğŸš€ ä½“éªŒæ¨¡å¼é…ç½®æ·±åº¦å½±å“åˆ†æå·¥å…·å¯åŠ¨...")

    try:
        # åˆ›å»ºåˆ†æå™¨å®ä¾‹ - é»˜è®¤ä½¿ç”¨å¤šæ–‡ä»¶æ¨¡å¼
        analyzer = ExperienceConfigAnalyzer()

        # è¿è¡Œæ·±åº¦åˆ†æ
        results = analyzer.run_complete_analysis()

        print(f"\nâœ¨ æ·±åº¦åˆ†æå®Œæˆæ€»ç»“:")
        print(f"   ğŸ“‹ å¤„ç†æ•°æ®: {results['data_summary']['total_records']:,}æ¡è®°å½•")
        print(f"   ğŸ¯ å”¯ä¸€é…ç½®: {results['data_summary']['unique_configs']}ç§")
        print(f"   ğŸ“Š ç›®æ ‡æŒ‡æ ‡: {results['data_summary']['target_metrics']}ä¸ª")
        print(f"   ğŸ¤– æœ€ä½³æ¨¡å‹RÂ²: {results['analysis_results']['best_model_r2']:.3f}")

        analysis_status = results['analysis_results']
        print(f"\nğŸ” åˆ†ææ¨¡å—çŠ¶æ€:")
        print(f"   âœ… åŸºç¡€åˆ†æ: {'å®Œæˆ' if analysis_status['basic_analysis'] else 'æœªå®Œæˆ'}")
        print(f"   âœ… é«˜çº§åˆ†æ: {'å®Œæˆ' if analysis_status['advanced_analysis'] else 'æœªå®Œæˆ'}")
        print(f"   âœ… åŠ¨æ€åˆ†æ: {'å®Œæˆ' if analysis_status['dynamic_analysis'] else 'æœªå®Œæˆ'}")
        print(f"   âœ… æœºåˆ¶åˆ†æ: {'å®Œæˆ' if analysis_status['mechanism_analysis'] else 'æœªå®Œæˆ'}")

        # è¾“å‡ºæ•°æ®æ¥æºä¿¡æ¯
        if hasattr(analyzer, 'csv_files') and analyzer.csv_files:
            print(f"\nğŸ“‚ æ•°æ®æ¥æº: {len(analyzer.csv_files)}ä¸ªCSVæ–‡ä»¶")
            for i, csv_file in enumerate(analyzer.csv_files[:5]):  # æœ€å¤šæ˜¾ç¤ºå‰5ä¸ª
                print(f"   {i+1}. {Path(csv_file).name}")
            if len(analyzer.csv_files) > 5:
                print(f"   ... è¿˜æœ‰{len(analyzer.csv_files)-5}ä¸ªæ–‡ä»¶")

    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        raise


def analyze_specific_directory(directory_path: str):
    """åˆ†ææŒ‡å®šç›®å½•çš„CSVæ–‡ä»¶"""
    print(f"ğŸ¯ åˆ†ææŒ‡å®šç›®å½•: {directory_path}")

    try:
        # åˆ›å»ºåˆ†æå™¨å®ä¾‹æŒ‡å®šç›®å½•
        analyzer = ExperienceConfigAnalyzer(csv_directory=directory_path)

        # è¿è¡Œæ·±åº¦åˆ†æ
        results = analyzer.run_complete_analysis()

        return results

    except Exception as e:
        print(f"âŒ æŒ‡å®šç›®å½•åˆ†æå¤±è´¥: {str(e)}")
        raise


def analyze_single_file(file_path: str):
    """åˆ†æå•ä¸ªCSVæ–‡ä»¶"""
    print(f"ğŸ“„ åˆ†æå•ä¸ªæ–‡ä»¶: {file_path}")

    try:
        # åˆ›å»ºåˆ†æå™¨å®ä¾‹æŒ‡å®šæ–‡ä»¶
        analyzer = ExperienceConfigAnalyzer(csv_path=file_path)

        # è¿è¡Œæ·±åº¦åˆ†æ
        results = analyzer.run_complete_analysis()

        return results

    except Exception as e:
        print(f"âŒ å•æ–‡ä»¶åˆ†æå¤±è´¥: {str(e)}")
        raise


if __name__ == "__main__":
    main()