#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½“éªŒæ¨¡å¼é…ç½®[x,y,z]å¯¹TileMatchæŒ‡æ ‡å½±å“çš„æ·±åº¦åˆ†æå·¥å…·
å‡çº§ç‰ˆï¼šä½ç½®ç‹¬ç«‹æ•ˆåº”ã€äº¤äº’æ•ˆåº”ã€åŠ¨æ€å½±å“ã€æœºåˆ¶åˆ†æ

ä½œè€…: Claude Code Assistant
å‡çº§æ—¶é—´: 2025-01-27
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.preprocessing import StandardScaler
from scipy import stats
from scipy.stats import pearsonr, spearmanr
import warnings
import os
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import json

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œæ ·å¼
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")
warnings.filterwarnings('ignore')

class ExperienceConfigAnalyzer:
    """ä½“éªŒæ¨¡å¼é…ç½®åˆ†æå™¨ - æ·±åº¦åˆ†æ[x,y,z]ä½ç½®å½±å“æœºåˆ¶"""

    def __init__(self, csv_path: str = None, csv_directory: str = None):
        """åˆå§‹åŒ–åˆ†æå™¨

        Args:
            csv_path: å•ä¸ªCSVæ•°æ®æ–‡ä»¶è·¯å¾„
            csv_directory: CSVæ–‡ä»¶ç›®å½•è·¯å¾„ï¼Œç”¨äºæ‰¹é‡å¤„ç†å¤šä¸ªæ–‡ä»¶
        """
        self.csv_path = csv_path
        self.csv_directory = csv_directory or self._find_csv_directory()
        self.data = None
        self.features = None
        self.csv_files = []
        self.target_metrics = [
            'DifficultyScore', 'PeakDockCount', 'PressureValueMean',
            'PressureValueMax', 'PressureValueStdDev', 'FinalDifficulty',
            'TotalMoves', 'InitialMinCost', 'DifficultyPosition'
        ]
        self.results = {}

    def _find_csv_directory(self) -> str:
        """è‡ªåŠ¨æŸ¥æ‰¾CSVæ–‡ä»¶ç›®å½•"""
        current_dir = Path(__file__).parent

        # ä¼˜å…ˆæŸ¥æ‰¾analysis_chartsç›®å½•
        analysis_charts_dir = current_dir / "BattleAnalysisResults" / "analysis_charts"
        if analysis_charts_dir.exists():
            csv_files = list(analysis_charts_dir.glob("*.csv"))
            if csv_files:
                print(f"ğŸ” æ£€æµ‹åˆ°analysis_chartsç›®å½•ï¼ŒåŒ…å«{len(csv_files)}ä¸ªCSVæ–‡ä»¶")
                return str(analysis_charts_dir)

        # æŸ¥æ‰¾BattleAnalysisResultsæ ¹ç›®å½•
        results_dir = current_dir / "BattleAnalysisResults"
        if results_dir.exists():
            csv_files = list(results_dir.glob("*.csv"))
            if csv_files:
                print(f"ğŸ” æ£€æµ‹åˆ°BattleAnalysisResultsç›®å½•ï¼ŒåŒ…å«{len(csv_files)}ä¸ªCSVæ–‡ä»¶")
                return str(results_dir)

        # æŸ¥æ‰¾å½“å‰ç›®å½•
        csv_files = list(current_dir.glob("*.csv"))
        if csv_files:
            print(f"ğŸ” æ£€æµ‹åˆ°å½“å‰ç›®å½•ï¼ŒåŒ…å«{len(csv_files)}ä¸ªCSVæ–‡ä»¶")
            return str(current_dir)

        raise FileNotFoundError("æœªæ‰¾åˆ°åŒ…å«CSVæ–‡ä»¶çš„ç›®å½•ï¼Œè¯·ç¡®ä¿æ•°æ®æ–‡ä»¶å­˜åœ¨")

    def load_and_preprocess(self) -> 'ExperienceConfigAnalyzer':
        """åŠ è½½æ•°æ®å¹¶è¿›è¡Œé¢„å¤„ç†"""
        print("ğŸ“Š åŠ è½½å’Œé¢„å¤„ç†æ•°æ®...")

        if self.csv_path:
            # å•æ–‡ä»¶æ¨¡å¼
            self.data = pd.read_csv(self.csv_path, encoding='utf-8')
            print(f"âœ… å•æ–‡ä»¶æ•°æ®åŠ è½½å®Œæˆ: {len(self.data)}æ¡è®°å½•")
        else:
            # å¤šæ–‡ä»¶æ¨¡å¼ - æ‰¹é‡åŠ è½½
            self._load_multiple_csv_files()

        # è§£æä½“éªŒæ¨¡å¼é…ç½®
        self._parse_experience_config()

        # ç”Ÿæˆç‰¹å¾å·¥ç¨‹
        self._create_features()

        # æ•°æ®æ¸…æ´—
        self._clean_data()

        print(f"ğŸ“ˆ é¢„å¤„ç†å®Œæˆ: {len(self.data)}æ¡æœ‰æ•ˆè®°å½•, {len(self.features.columns)}ä¸ªç‰¹å¾")
        return self

    def _load_multiple_csv_files(self):
        """æ‰¹é‡åŠ è½½å¤šä¸ªCSVæ–‡ä»¶ï¼Œä½¿ç”¨æœ€ç®€å•çš„æ–¹æ¡ˆ"""
        csv_dir = Path(self.csv_directory)
        self.csv_files = sorted(list(csv_dir.glob("*.csv")))

        if not self.csv_files:
            raise FileNotFoundError(f"åœ¨ç›®å½• {csv_dir} ä¸­æœªæ‰¾åˆ°CSVæ–‡ä»¶")

        print(f"ğŸ” å‘ç°{len(self.csv_files)}ä¸ªCSVæ–‡ä»¶ï¼Œå‡†å¤‡æ‰¹é‡åŠ è½½...")

        # ç®€å•ç›´æ¥åŠ è½½ï¼Œä¸€ä¸ªæ–‡ä»¶ä¸€ä¸ªæ–‡ä»¶å¤„ç†
        data_list = []
        total_rows = 0

        for i, csv_file in enumerate(self.csv_files):
            print(f"   æ­£åœ¨åŠ è½½ {csv_file.name}... ({i+1}/{len(self.csv_files)})")

            try:
                # ç›´æ¥è¯»å–ï¼Œä¸åˆ†å—
                file_data = pd.read_csv(csv_file, encoding='utf-8')
                data_list.append(file_data)
                total_rows += len(file_data)
                print(f"     âœ… {csv_file.name}: {len(file_data)}è¡Œ")

            except Exception as e:
                print(f"     âŒ åŠ è½½{csv_file.name}å¤±è´¥: {str(e)}")
                continue

        if not data_list:
            raise ValueError("æ‰€æœ‰CSVæ–‡ä»¶éƒ½åŠ è½½å¤±è´¥")

        # åˆå¹¶æ‰€æœ‰æ•°æ®
        print("ğŸ”— åˆå¹¶æ‰€æœ‰æ•°æ®...")
        self.data = pd.concat(data_list, ignore_index=True)

        print(f"âœ… æ‰¹é‡æ•°æ®åŠ è½½å®Œæˆ: æ€»è®¡{total_rows}æ¡è®°å½•ï¼Œåˆå¹¶å{len(self.data)}æ¡è®°å½•")

    def _parse_experience_config(self):
        """è§£æä½“éªŒæ¨¡å¼é…ç½®[1,2,3]æ ¼å¼ï¼Œä½¿ç”¨æœ€ç®€å•çš„å®ç°"""
        # æ£€æŸ¥ExperienceModeåˆ—æ˜¯å¦å­˜åœ¨
        if 'ExperienceMode' not in self.data.columns:
            print("âš ï¸ æœªæ‰¾åˆ°ExperienceModeåˆ—ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            self.data['pos1'] = 1
            self.data['pos2'] = 2
            self.data['pos3'] = 3
            return

        print("ğŸ”§ è§£æä½“éªŒæ¨¡å¼é…ç½®...")

        # ä½¿ç”¨æœ€ç®€å•çš„è§£ææ–¹å¼ï¼Œé¿å…å¤æ‚çš„applyæ“ä½œ
        pos1_list = []
        pos2_list = []
        pos3_list = []

        for config_str in self.data['ExperienceMode']:
            try:
                if pd.isna(config_str):
                    pos1_list.append(1)
                    pos2_list.append(2)
                    pos3_list.append(3)
                    continue

                # ç®€å•å­—ç¬¦ä¸²å¤„ç†
                clean_str = str(config_str).strip('[]"()')
                numbers = [int(x.strip()) for x in clean_str.split(',')]

                if len(numbers) >= 3:
                    pos1_list.append(numbers[0])
                    pos2_list.append(numbers[1])
                    pos3_list.append(numbers[2])
                else:
                    pos1_list.append(1)
                    pos2_list.append(2)
                    pos3_list.append(3)

            except:
                pos1_list.append(1)
                pos2_list.append(2)
                pos3_list.append(3)

        # ç›´æ¥èµ‹å€¼ï¼Œä¸ä½¿ç”¨apply
        self.data['pos1'] = pos1_list
        self.data['pos2'] = pos2_list
        self.data['pos3'] = pos3_list

        # éªŒè¯è§£æç»“æœ
        unique_configs = len(self.data[['pos1', 'pos2', 'pos3']].drop_duplicates())
        print(f"   è§£æå®Œæˆï¼šå‘ç°{unique_configs}ç§ä¸åŒé…ç½®")

    def _create_features(self):
        """åˆ›å»ºç‰¹å¾å·¥ç¨‹"""
        # åŸºç¡€ä½ç½®ç‰¹å¾
        features_df = self.data[['pos1', 'pos2', 'pos3']].copy()

        # ç»Ÿè®¡ç‰¹å¾
        features_df['config_mean'] = (self.data['pos1'] + self.data['pos2'] + self.data['pos3']) / 3
        features_df['config_std'] = np.sqrt(((self.data['pos1'] - features_df['config_mean'])**2 +
                                            (self.data['pos2'] - features_df['config_mean'])**2 +
                                            (self.data['pos3'] - features_df['config_mean'])**2) / 3)
        features_df['config_range'] = np.maximum.reduce([self.data['pos1'], self.data['pos2'], self.data['pos3']]) - \
                                     np.minimum.reduce([self.data['pos1'], self.data['pos2'], self.data['pos3']])
        features_df['config_sum'] = self.data['pos1'] + self.data['pos2'] + self.data['pos3']

        # äº¤äº’ç‰¹å¾
        features_df['pos1_pos2'] = self.data['pos1'] * self.data['pos2']
        features_df['pos1_pos3'] = self.data['pos1'] * self.data['pos3']
        features_df['pos2_pos3'] = self.data['pos2'] * self.data['pos3']
        features_df['pos_product'] = self.data['pos1'] * self.data['pos2'] * self.data['pos3']

        # åºåˆ—æ¨¡å¼ç‰¹å¾
        features_df['is_increasing'] = ((self.data['pos1'] <= self.data['pos2']) &
                                      (self.data['pos2'] <= self.data['pos3'])).astype(int)
        features_df['is_decreasing'] = ((self.data['pos1'] >= self.data['pos2']) &
                                      (self.data['pos2'] >= self.data['pos3'])).astype(int)
        features_df['is_uniform'] = ((self.data['pos1'] == self.data['pos2']) &
                                   (self.data['pos2'] == self.data['pos3'])).astype(int)

        # æå€¼ç‰¹å¾
        features_df['has_extreme_low'] = ((self.data['pos1'] == 1) | (self.data['pos2'] == 1) | (self.data['pos3'] == 1)).astype(int)
        features_df['has_extreme_high'] = ((self.data['pos1'] == 9) | (self.data['pos2'] == 9) | (self.data['pos3'] == 9)).astype(int)

        self.features = features_df

    def _clean_data(self):
        """æ•°æ®æ¸…æ´—ï¼Œå¢å¼ºè¾¹ç•Œæ¡ä»¶æ£€æŸ¥"""
        initial_count = len(self.data)
        print(f"ğŸ§¹ å¼€å§‹æ•°æ®æ¸…æ´—ï¼Œåˆå§‹è®°å½•æ•°: {initial_count}")

        # ä¿å­˜åŸå§‹ç´¢å¼•ç”¨äºç‰¹å¾æ•°æ®åŒæ­¥
        original_indices = self.data.index.copy()

        # æ£€æŸ¥GameCompletedåˆ—æ˜¯å¦å­˜åœ¨
        if 'GameCompleted' in self.data.columns:
            # ç§»é™¤GameCompleted=Falseçš„è®°å½•
            completed_mask = (self.data['GameCompleted'] == True) | (self.data['GameCompleted'] == 'True')
            self.data = self.data[completed_mask].copy()
            print(f"   å®Œæˆæ¸¸æˆè¿‡æ»¤: {len(self.data)}æ¡è®°å½•")
        else:
            print("   âš ï¸ æœªæ‰¾åˆ°GameCompletedåˆ—ï¼Œè·³è¿‡å®ŒæˆçŠ¶æ€è¿‡æ»¤")

        # ç§»é™¤ç›®æ ‡æŒ‡æ ‡ä¸ºç©ºå€¼çš„è®°å½•
        metrics_found = []
        for metric in self.target_metrics:
            if metric in self.data.columns:
                metrics_found.append(metric)
                before_count = len(self.data)
                self.data = self.data[self.data[metric].notna()].copy()
                if before_count != len(self.data):
                    print(f"   {metric}ç©ºå€¼è¿‡æ»¤: {len(self.data)}æ¡è®°å½•")

        if not metrics_found:
            print("   âš ï¸ æœªæ‰¾åˆ°ä»»ä½•ç›®æ ‡æŒ‡æ ‡åˆ—ï¼Œæ•°æ®å¯èƒ½å­˜åœ¨é—®é¢˜")
        else:
            print(f"   æ‰¾åˆ°{len(metrics_found)}ä¸ªæœ‰æ•ˆæŒ‡æ ‡: {', '.join(metrics_found)}")

        # ç§»é™¤é…ç½®è§£æå¼‚å¸¸çš„è®°å½•
        invalid_config_mask = (
            (self.data['pos1'] < 1) | (self.data['pos1'] > 9) |
            (self.data['pos2'] < 1) | (self.data['pos2'] > 9) |
            (self.data['pos3'] < 1) | (self.data['pos3'] > 9)
        )
        if invalid_config_mask.any():
            invalid_count = invalid_config_mask.sum()
            self.data = self.data[~invalid_config_mask].copy()
            print(f"   é…ç½®å¼‚å¸¸è¿‡æ»¤: ç§»é™¤{invalid_count}æ¡ï¼Œå‰©ä½™{len(self.data)}æ¡è®°å½•")

        # å®‰å…¨çš„ç‰¹å¾æ•°æ®é‡æ–°ç´¢å¼•
        try:
            # æ‰¾åˆ°ç‰¹å¾æ•°æ®å’Œæ¸…æ´—åæ•°æ®çš„äº¤é›†ç´¢å¼•
            valid_indices = self.data.index.intersection(self.features.index)

            if len(valid_indices) == len(self.data):
                # å®Œç¾åŒ¹é…ï¼Œç›´æ¥é‡æ–°ç´¢å¼•
                self.features = self.features.loc[valid_indices].copy()
            else:
                # ä¸å®Œç¾åŒ¹é…ï¼Œé‡æ–°ç”Ÿæˆç‰¹å¾æ•°æ®ä»¥ä¿è¯ä¸€è‡´æ€§
                print(f"   âš ï¸ ç‰¹å¾æ•°æ®ç´¢å¼•ä¸åŒ¹é…ï¼Œé‡æ–°ç”Ÿæˆç‰¹å¾")
                self._create_features()

        except (KeyError, IndexError) as e:
            # ç´¢å¼•ä¸¥é‡ä¸åŒ¹é…ï¼Œé‡æ–°ç”Ÿæˆ
            print(f"   âŒ ç‰¹å¾æ•°æ®ç´¢å¼•é”™è¯¯ï¼Œé‡æ–°ç”Ÿæˆ: {str(e)}")
            self._create_features()

        final_count = len(self.data)
        filtered_ratio = (initial_count - final_count) / initial_count * 100 if initial_count > 0 else 0
        print(f"ğŸ§¹ æ•°æ®æ¸…æ´—å®Œæˆ: {initial_count} -> {final_count}æ¡è®°å½• (è¿‡æ»¤{filtered_ratio:.1f}%)")

        # æ•°æ®è´¨é‡æ£€æŸ¥
        if final_count < initial_count * 0.1:  # å¦‚æœè¿‡æ»¤æ‰è¶…è¿‡90%çš„æ•°æ®
            print(f"âš ï¸ è­¦å‘Š: è¿‡æ»¤æ¯”ä¾‹è¿‡é«˜({filtered_ratio:.1f}%)ï¼Œè¯·æ£€æŸ¥æ•°æ®è´¨é‡")

        if final_count < 100:  # å¦‚æœæœ€ç»ˆæ•°æ®é‡è¿‡å°‘
            print(f"âš ï¸ è­¦å‘Š: æœ‰æ•ˆæ•°æ®é‡è¿‡å°‘({final_count}æ¡)ï¼Œåˆ†æç»“æœå¯èƒ½ä¸å¯é ")

    def correlation_analysis(self) -> Dict:
        """åŸºç¡€ç›¸å…³æ€§åˆ†æ"""
        print("ğŸ”— æ‰§è¡ŒåŸºç¡€ç›¸å…³æ€§åˆ†æ...")

        correlations = {}

        # ä½ç½®ç‹¬ç«‹ç›¸å…³æ€§
        position_corrs = {}
        for pos in ['pos1', 'pos2', 'pos3']:
            pos_corrs = {}
            for metric in self.target_metrics:
                if metric in self.data.columns:
                    corr, p_value = pearsonr(self.features[pos], self.data[metric])
                    pos_corrs[metric] = {
                        'correlation': corr,
                        'p_value': p_value,
                        'significant': p_value < 0.05,
                        'effect_size': abs(corr)
                    }
            position_corrs[pos] = pos_corrs

        correlations['position_correlations'] = position_corrs

        # ç‰¹å¾é‡è¦æ€§
        feature_importance = {}
        for metric in self.target_metrics:
            if metric in self.data.columns:
                rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
                rf.fit(self.features, self.data[metric])
                importance_dict = dict(zip(self.features.columns, rf.feature_importances_))
                feature_importance[metric] = dict(sorted(importance_dict.items(),
                                                       key=lambda x: x[1], reverse=True))

        correlations['feature_importance'] = feature_importance
        self.results['correlations'] = correlations
        print("âœ… åŸºç¡€ç›¸å…³æ€§åˆ†æå®Œæˆ")
        return correlations

    def position_independent_analysis(self) -> Dict:
        """ä½ç½®ç‹¬ç«‹å½±å“åˆ†æ - æ§åˆ¶å…¶ä»–å˜é‡åˆ†æå•ä¸ªä½ç½®çš„çº¯å‡€æ•ˆåº”"""
        print("ğŸ¯ æ‰§è¡Œä½ç½®ç‹¬ç«‹å½±å“åˆ†æ...")

        independent_effects = {}

        for pos in ['pos1', 'pos2', 'pos3']:
            pos_effects = {}

            for metric in ['DifficultyScore', 'PeakDockCount', 'PressureValueMean']:
                if metric in self.data.columns:
                    # è®¡ç®—è¾¹é™…æ•ˆåº”ï¼šposæ¯å˜åŒ–1å•ä½å¯¹metricçš„å½±å“
                    marginal_effects = []

                    # åˆ†æä¸åŒæ•°å€¼åŒºé—´çš„æ•ˆåº”
                    for value in range(1, 10):
                        subset = self.data[self.features[pos] == value]
                        if len(subset) > 5:  # ç¡®ä¿æ ·æœ¬é‡è¶³å¤Ÿ
                            avg_metric = subset[metric].mean()
                            marginal_effects.append((value, avg_metric))

                    if len(marginal_effects) > 2:
                        # è®¡ç®—è¾¹é™…é€’å¢æ•ˆåº”
                        marginal_diffs = []
                        for i in range(1, len(marginal_effects)):
                            diff = marginal_effects[i][1] - marginal_effects[i-1][1]
                            marginal_diffs.append(diff)

                        pos_effects[metric] = {
                            'marginal_effects': marginal_effects,
                            'avg_marginal_diff': np.mean(marginal_diffs),
                            'marginal_volatility': np.std(marginal_diffs)
                        }

            independent_effects[pos] = pos_effects

        self.results['independent_effects'] = independent_effects
        print("âœ… ä½ç½®ç‹¬ç«‹å½±å“åˆ†æå®Œæˆ")
        return independent_effects

    def interaction_analysis(self) -> Dict:
        """ä½ç½®äº¤äº’æ•ˆåº”åˆ†æ - åˆ†æä½ç½®é—´ç›¸äº’ä½œç”¨"""
        print("ğŸ”„ æ‰§è¡Œä½ç½®äº¤äº’æ•ˆåº”åˆ†æ...")

        interaction_effects = {}

        # åŒä½ç½®äº¤äº’åˆ†æ
        for pos_pair in [('pos1', 'pos2'), ('pos1', 'pos3'), ('pos2', 'pos3')]:
            pair_key = f"{pos_pair[0]}Ã—{pos_pair[1]}"
            pair_effects = {}

            for metric in ['DifficultyScore', 'PeakDockCount']:
                if metric in self.data.columns:
                    # åˆ›å»ºäº¤äº’ç‰¹å¾
                    interaction_feature = self.features[pos_pair[0]] * self.features[pos_pair[1]]

                    # æ¯”è¾ƒæœ‰äº¤äº’é¡¹vsæ— äº¤äº’é¡¹çš„æ¨¡å‹æ€§èƒ½
                    X_base = self.features[[pos_pair[0], pos_pair[1]]]
                    X_inter = X_base.copy()
                    X_inter['interaction'] = interaction_feature

                    # åŸºç¡€æ¨¡å‹
                    rf_base = RandomForestRegressor(n_estimators=50, random_state=42)
                    rf_base.fit(X_base, self.data[metric])
                    r2_base = rf_base.score(X_base, self.data[metric])

                    # äº¤äº’æ¨¡å‹
                    rf_inter = RandomForestRegressor(n_estimators=50, random_state=42)
                    rf_inter.fit(X_inter, self.data[metric])
                    r2_inter = rf_inter.score(X_inter, self.data[metric])

                    pair_effects[metric] = {
                        'r2_base': r2_base,
                        'r2_interaction': r2_inter,
                        'interaction_gain': r2_inter - r2_base,
                        'interaction_strength': abs(r2_inter - r2_base)
                    }

            interaction_effects[pair_key] = pair_effects

        self.results['interaction_effects'] = interaction_effects
        print("âœ… ä½ç½®äº¤äº’æ•ˆåº”åˆ†æå®Œæˆ")
        return interaction_effects

    def dynamic_impact_analysis(self) -> Dict:
        """åŠ¨æ€å½±å“åˆ†æ - åˆ†æä½ç½®åœ¨ä¸åŒæ¸¸æˆé˜¶æ®µçš„å·®å¼‚åŒ–å½±å“"""
        print("â±ï¸ æ‰§è¡ŒåŠ¨æ€å½±å“åˆ†æ...")

        dynamic_effects = {}

        # è§£æDockAfterTrioMatchåºåˆ—æ•°æ®
        dock_sequences = []
        for dock_str in self.data['DockAfterTrioMatch'].fillna(''):
            if dock_str:
                try:
                    dock_values = [int(x) for x in str(dock_str).split(',')]
                    dock_sequences.append(dock_values)
                except:
                    dock_sequences.append([])
            else:
                dock_sequences.append([])

        # åˆ†æå„ä½ç½®å¯¹ä¸åŒæ¸¸æˆé˜¶æ®µçš„å½±å“
        for pos in ['pos1', 'pos2', 'pos3']:
            pos_dynamic = {}

            # åˆ†æå¯¹æ¸¸æˆå‰æœŸã€ä¸­æœŸã€åæœŸçš„å·®å¼‚åŒ–å½±å“
            stage_effects = {'early': [], 'middle': [], 'late': []}

            for i, dock_seq in enumerate(dock_sequences):
                if len(dock_seq) >= 6:  # ç¡®ä¿åºåˆ—è¶³å¤Ÿé•¿
                    seq_len = len(dock_seq)
                    early_avg = np.mean(dock_seq[:seq_len//3])
                    middle_avg = np.mean(dock_seq[seq_len//3:2*seq_len//3])
                    late_avg = np.mean(dock_seq[2*seq_len//3:])

                    pos_value = self.features.iloc[i][pos]
                    stage_effects['early'].append((pos_value, early_avg))
                    stage_effects['middle'].append((pos_value, middle_avg))
                    stage_effects['late'].append((pos_value, late_avg))

            # è®¡ç®—å„é˜¶æ®µçš„ç›¸å…³æ€§
            for stage, values in stage_effects.items():
                if len(values) > 10:
                    pos_vals, dock_vals = zip(*values)
                    corr, _ = pearsonr(pos_vals, dock_vals)
                    pos_dynamic[f'{stage}_correlation'] = corr

            dynamic_effects[pos] = pos_dynamic

        self.results['dynamic_effects'] = dynamic_effects
        print("âœ… åŠ¨æ€å½±å“åˆ†æå®Œæˆ")
        return dynamic_effects

    def mechanism_analysis(self) -> Dict:
        """å½±å“æœºåˆ¶åˆ†æ - åˆ†æä½ç½®å½±å“æŒ‡æ ‡çš„ä¸­ä»‹è·¯å¾„"""
        print("ğŸ” æ‰§è¡Œå½±å“æœºåˆ¶åˆ†æ...")

        mechanism_effects = {}

        # å¯¹äºæ¯ä¸ªä½ç½®ï¼Œåˆ†æå…¶å¯¹DifficultyScoreçš„ä¸­ä»‹è·¯å¾„
        for pos in ['pos1', 'pos2', 'pos3']:
            pos_mechanism = {}

            # ç›´æ¥æ•ˆåº”ï¼špos -> DifficultyScore
            if 'DifficultyScore' in self.data.columns:
                direct_corr, _ = pearsonr(self.features[pos], self.data['DifficultyScore'])
                pos_mechanism['direct_effect'] = direct_corr

                # ä¸­ä»‹æ•ˆåº”åˆ†æï¼špos -> ä¸­ä»‹å˜é‡ -> DifficultyScore
                mediators = ['PressureValueMean', 'PeakDockCount', 'PressureValueMax']
                mediation_effects = {}

                for mediator in mediators:
                    if mediator in self.data.columns:
                        # pos -> mediator
                        corr_pm, _ = pearsonr(self.features[pos], self.data[mediator])
                        # mediator -> DifficultyScore
                        corr_md, _ = pearsonr(self.data[mediator], self.data['DifficultyScore'])
                        # ä¸­ä»‹æ•ˆåº”å¼ºåº¦ = ä¸¤ä¸ªç›¸å…³ç³»æ•°çš„ä¹˜ç§¯
                        mediation_strength = corr_pm * corr_md

                        mediation_effects[mediator] = {
                            'pos_to_mediator': corr_pm,
                            'mediator_to_target': corr_md,
                            'mediation_strength': mediation_strength
                        }

                pos_mechanism['mediation_effects'] = mediation_effects

                # æ‰¾å‡ºæœ€å¼ºçš„ä¸­ä»‹è·¯å¾„
                if mediation_effects:
                    strongest_mediator = max(mediation_effects.items(),
                                           key=lambda x: abs(x[1]['mediation_strength']))
                    pos_mechanism['strongest_mediation_path'] = {
                        'mediator': strongest_mediator[0],
                        'strength': strongest_mediator[1]['mediation_strength']
                    }

            mechanism_effects[pos] = pos_mechanism

        self.results['mechanism_effects'] = mechanism_effects
        print("âœ… å½±å“æœºåˆ¶åˆ†æå®Œæˆ")
        return mechanism_effects

    def value_specific_analysis(self, target_value: int = None) -> Dict:
        """å•ä¸€æ•°å€¼æ·±åº¦åˆ†æ - åˆ†æç‰¹å®šæ•°å€¼(å¦‚x=5)åœ¨ä¸åŒä½ç½®çš„å®Œæ•´å½±å“"""
        print(f"ğŸ¯ æ‰§è¡Œå•ä¸€æ•°å€¼æ·±åº¦åˆ†æ (ç›®æ ‡æ•°å€¼: {target_value or 'å…¨éƒ¨'})...")

        value_effects = {}
        values_to_analyze = [target_value] if target_value else range(1, 10)

        for value in values_to_analyze:
            value_key = f"value_{value}"
            value_data = {}

            for pos in ['pos1', 'pos2', 'pos3']:
                pos_data = {}

                # ç­›é€‰è¯¥æ•°å€¼åœ¨è¯¥ä½ç½®çš„æ•°æ®
                mask = self.features[pos] == value
                subset = self.data[mask]

                if len(subset) < 10:  # æ ·æœ¬é‡å¤ªå°‘è·³è¿‡
                    continue

                # DifficultyScoreå½±å“åˆ†æ
                if 'DifficultyScore' in subset.columns:
                    difficulty_stats = {
                        'mean': subset['DifficultyScore'].mean(),
                        'std': subset['DifficultyScore'].std(),
                        'median': subset['DifficultyScore'].median(),
                        'min': subset['DifficultyScore'].min(),
                        'max': subset['DifficultyScore'].max(),
                        'count': len(subset)
                    }
                    pos_data['difficulty_impact'] = difficulty_stats

                # èƒœç‡åˆ†æ
                if 'GameCompleted' in subset.columns:
                    win_rate = (subset['GameCompleted'] == True).mean()
                    pos_data['win_rate'] = {
                        'success_rate': win_rate,
                        'failure_rate': 1 - win_rate,
                        'total_games': len(subset)
                    }

                # DockAfterTrioMatchåºåˆ—åˆ†æ
                if 'DockAfterTrioMatch' in subset.columns:
                    dock_analysis = self._analyze_dock_sequences(subset['DockAfterTrioMatch'])
                    pos_data['dock_impact'] = dock_analysis

                # PressureValuesåˆ†æ
                pressure_analysis = {}
                for pressure_col in ['PressureValueMean', 'PressureValueMax', 'PressureValueStdDev']:
                    if pressure_col in subset.columns:
                        pressure_analysis[pressure_col] = {
                            'mean': subset[pressure_col].mean(),
                            'std': subset[pressure_col].std(),
                            'median': subset[pressure_col].median()
                        }
                pos_data['pressure_impact'] = pressure_analysis

                value_data[pos] = pos_data

            value_effects[value_key] = value_data

        self.results['value_specific_effects'] = value_effects
        print("âœ… å•ä¸€æ•°å€¼æ·±åº¦åˆ†æå®Œæˆ")
        return value_effects

    def _analyze_dock_sequences(self, dock_series) -> Dict:
        """åˆ†æDockAfterTrioMatchåºåˆ—çš„è¾…åŠ©æ–¹æ³•"""
        sequences = []
        for dock_str in dock_series.fillna(''):
            if dock_str:
                try:
                    dock_values = [int(x) for x in str(dock_str).split(',')]
                    sequences.append(dock_values)
                except:
                    continue

        if not sequences:
            return {}

        # è®¡ç®—åºåˆ—ç»Ÿè®¡
        seq_lengths = [len(seq) for seq in sequences]
        avg_length = np.mean(seq_lengths)

        # åˆ†æä¸åŒé˜¶æ®µçš„å¹³å‡Dockå€¼
        early_vals, middle_vals, late_vals = [], [], []
        for seq in sequences:
            if len(seq) >= 6:
                third = len(seq) // 3
                early_vals.extend(seq[:third])
                middle_vals.extend(seq[third:2*third])
                late_vals.extend(seq[2*third:])

        return {
            'avg_sequence_length': avg_length,
            'total_sequences': len(sequences),
            'phase_analysis': {
                'early_phase': {'mean': np.mean(early_vals) if early_vals else 0, 'count': len(early_vals)},
                'middle_phase': {'mean': np.mean(middle_vals) if middle_vals else 0, 'count': len(middle_vals)},
                'late_phase': {'mean': np.mean(late_vals) if late_vals else 0, 'count': len(late_vals)}
            }
        }

    def gradient_effect_analysis(self) -> Dict:
        """æ•°å€¼æ¢¯åº¦æ•ˆåº”åˆ†æ - åˆ†æ1-9æ•°å€¼çš„è¿ç»­å½±å“å˜åŒ–"""
        print("ğŸ“ˆ æ‰§è¡Œæ•°å€¼æ¢¯åº¦æ•ˆåº”åˆ†æ...")

        gradient_effects = {}

        for pos in ['pos1', 'pos2', 'pos3']:
            pos_gradients = {}

            # ä¸»è¦æŒ‡æ ‡çš„æ¢¯åº¦åˆ†æ
            for metric in ['DifficultyScore', 'PeakDockCount', 'PressureValueMean']:
                if metric in self.data.columns:

                    # è®¡ç®—æ¯ä¸ªæ•°å€¼çš„å¹³å‡æŒ‡æ ‡å€¼
                    value_means = []
                    value_counts = []

                    for value in range(1, 10):
                        subset = self.data[self.features[pos] == value]
                        if len(subset) > 0:
                            value_means.append(subset[metric].mean())
                            value_counts.append(len(subset))
                        else:
                            value_means.append(np.nan)
                            value_counts.append(0)

                    # è®¡ç®—æ¢¯åº¦(ç›¸é‚»æ•°å€¼é—´çš„å·®å€¼)
                    gradients = []
                    for i in range(1, len(value_means)):
                        if not (np.isnan(value_means[i]) or np.isnan(value_means[i-1])):
                            gradient = value_means[i] - value_means[i-1]
                            gradients.append(gradient)
                        else:
                            gradients.append(np.nan)

                    # è¯†åˆ«æœ€å¤§æ¢¯åº¦å˜åŒ–ç‚¹(ä¸´ç•Œç‚¹)
                    abs_gradients = [abs(g) for g in gradients if not np.isnan(g)]
                    if abs_gradients:
                        max_gradient_idx = gradients.index(max(gradients, key=abs))
                        critical_point = max_gradient_idx + 2  # +2å› ä¸ºæ¢¯åº¦æ˜¯å·®å€¼,å¯¹åº”åä¸€ä¸ªæ•°å€¼
                    else:
                        critical_point = None

                    pos_gradients[metric] = {
                        'value_means': value_means,
                        'gradients': gradients,
                        'critical_point': critical_point,
                        'max_gradient': max(abs_gradients) if abs_gradients else 0,
                        'avg_gradient': np.mean([g for g in gradients if not np.isnan(g)]) if gradients else 0
                    }

            gradient_effects[pos] = pos_gradients

        self.results['gradient_effects'] = gradient_effects
        print("âœ… æ•°å€¼æ¢¯åº¦æ•ˆåº”åˆ†æå®Œæˆ")
        return gradient_effects

    def dock_sequence_deep_analysis(self) -> Dict:
        """DockAfterTrioMatchåºåˆ—æ·±åº¦åˆ†æ"""
        print("ğŸš¢ æ‰§è¡ŒDockåºåˆ—æ·±åº¦åˆ†æ...")

        dock_deep_analysis = {}

        # è§£ææ‰€æœ‰åºåˆ—æ•°æ®
        all_sequences = []
        sequence_metadata = []

        for idx, row in self.data.iterrows():
            dock_str = row.get('DockAfterTrioMatch', '')
            if dock_str and str(dock_str) != 'nan':
                try:
                    dock_values = [int(x) for x in str(dock_str).split(',')]
                    all_sequences.append(dock_values)
                    sequence_metadata.append({
                        'pos1': self.features.loc[idx, 'pos1'],
                        'pos2': self.features.loc[idx, 'pos2'],
                        'pos3': self.features.loc[idx, 'pos3'],
                        'difficulty': row.get('DifficultyScore', 0),
                        'completed': row.get('GameCompleted', False)
                    })
                except:
                    continue

        if not all_sequences:
            return {}

        # æŒ‰ä½ç½®åˆ†ç»„åˆ†æ
        for pos in ['pos1', 'pos2', 'pos3']:
            pos_analysis = {}

            for value in range(1, 10):
                value_sequences = []
                value_metadata = []

                for i, meta in enumerate(sequence_metadata):
                    if meta[pos] == value:
                        value_sequences.append(all_sequences[i])
                        value_metadata.append(meta)

                if len(value_sequences) < 5:  # æ ·æœ¬é‡å¤ªå°‘
                    continue

                # åºåˆ—é•¿åº¦åˆ†æ
                lengths = [len(seq) for seq in value_sequences]

                # åºåˆ—æ¨¡å¼åˆ†æ
                patterns = self._identify_dock_patterns(value_sequences)

                # å±é™©æ—¶åˆ»åˆ†æ(Dock>=6çš„æ—¶åˆ»)
                danger_analysis = self._analyze_danger_moments(value_sequences)

                # æˆåŠŸç‡ä¸åºåˆ—ç‰¹å¾å…³ç³»
                success_rate = np.mean([meta['completed'] for meta in value_metadata])

                pos_analysis[f'value_{value}'] = {
                    'sequence_count': len(value_sequences),
                    'avg_length': np.mean(lengths),
                    'success_rate': success_rate,
                    'patterns': patterns,
                    'danger_analysis': danger_analysis
                }

            dock_deep_analysis[pos] = pos_analysis

        self.results['dock_deep_analysis'] = dock_deep_analysis
        print("âœ… Dockåºåˆ—æ·±åº¦åˆ†æå®Œæˆ")
        return dock_deep_analysis

    def _identify_dock_patterns(self, sequences) -> Dict:
        """è¯†åˆ«Dockåºåˆ—æ¨¡å¼"""
        if not sequences:
            return {}

        # è®¡ç®—å¹³å‡åºåˆ—
        max_length = max(len(seq) for seq in sequences)
        avg_sequence = []

        for i in range(max_length):
            values_at_i = [seq[i] for seq in sequences if len(seq) > i]
            if values_at_i:
                avg_sequence.append(np.mean(values_at_i))

        # è¯†åˆ«æ¨¡å¼ç±»å‹
        if len(avg_sequence) < 3:
            return {'pattern_type': 'insufficient_data'}

        # åˆ¤æ–­è¶‹åŠ¿
        early_avg = np.mean(avg_sequence[:len(avg_sequence)//3])
        late_avg = np.mean(avg_sequence[2*len(avg_sequence)//3:])

        if late_avg > early_avg + 0.5:
            pattern_type = 'increasing_pressure'
        elif late_avg < early_avg - 0.5:
            pattern_type = 'decreasing_pressure'
        else:
            pattern_type = 'stable_pressure'

        return {
            'pattern_type': pattern_type,
            'avg_sequence': avg_sequence[:10],  # åªä¿å­˜å‰10ä¸ªç‚¹
            'early_avg': early_avg,
            'late_avg': late_avg
        }

    def _analyze_danger_moments(self, sequences) -> Dict:
        """åˆ†æå±é™©æ—¶åˆ»(Dock>=6)"""
        if not sequences:
            return {}

        danger_counts = []
        max_danger_levels = []

        for seq in sequences:
            danger_count = sum(1 for val in seq if val >= 6)
            max_danger = max(seq) if seq else 0

            danger_counts.append(danger_count)
            max_danger_levels.append(max_danger)

        return {
            'avg_danger_moments': np.mean(danger_counts),
            'max_danger_level': np.mean(max_danger_levels),
            'danger_frequency': np.mean([d > 0 for d in danger_counts])
        }

    def pressure_dynamics_analysis(self) -> Dict:
        """å‹åŠ›åŠ¨æ€åˆ†æ"""
        print("âš¡ æ‰§è¡Œå‹åŠ›åŠ¨æ€åˆ†æ...")

        pressure_dynamics = {}

        pressure_columns = ['PressureValueMean', 'PressureValueMax', 'PressureValueStdDev']

        for pos in ['pos1', 'pos2', 'pos3']:
            pos_dynamics = {}

            for value in range(1, 10):
                subset = self.data[self.features[pos] == value]

                if len(subset) < 10:
                    continue

                value_dynamics = {}

                for pressure_col in pressure_columns:
                    if pressure_col in subset.columns:
                        pressure_values = subset[pressure_col].dropna()

                        if len(pressure_values) > 0:
                            # åŸºç¡€ç»Ÿè®¡
                            stats = {
                                'mean': pressure_values.mean(),
                                'std': pressure_values.std(),
                                'median': pressure_values.median(),
                                'q75': pressure_values.quantile(0.75),
                                'q95': pressure_values.quantile(0.95),
                                'max': pressure_values.max()
                            }

                            # å‹åŠ›åˆ†å¸ƒåˆ†æ
                            low_pressure = (pressure_values < pressure_values.quantile(0.33)).sum()
                            high_pressure = (pressure_values > pressure_values.quantile(0.67)).sum()

                            pressure_distribution = {
                                'low_pressure_count': low_pressure,
                                'high_pressure_count': high_pressure,
                                'extreme_pressure_count': (pressure_values > pressure_values.quantile(0.9)).sum()
                            }

                            value_dynamics[pressure_col] = {
                                'statistics': stats,
                                'distribution': pressure_distribution
                            }

                pos_dynamics[f'value_{value}'] = value_dynamics

            pressure_dynamics[pos] = pos_dynamics

        self.results['pressure_dynamics'] = pressure_dynamics
        print("âœ… å‹åŠ›åŠ¨æ€åˆ†æå®Œæˆ")
        return pressure_dynamics

    def pattern_analysis(self) -> Dict:
        """é…ç½®æ¨¡å¼åˆ†æ"""
        print("ğŸ¯ æ‰§è¡Œé…ç½®æ¨¡å¼åˆ†æ...")

        patterns = {}

        # åºåˆ—æ¨¡å¼åˆ†æ
        sequence_patterns = {
            'increasing': self.data[self.features['is_increasing'] == 1],
            'decreasing': self.data[self.features['is_decreasing'] == 1],
            'uniform': self.data[self.features['is_uniform'] == 1],
            'mixed': self.data[(self.features['is_increasing'] == 0) &
                             (self.features['is_decreasing'] == 0) &
                             (self.features['is_uniform'] == 0)]
        }

        pattern_stats = {}
        for pattern_name, pattern_data in sequence_patterns.items():
            if len(pattern_data) > 0:
                stats_dict = {}
                for metric in self.target_metrics:
                    if metric in pattern_data.columns:
                        stats_dict[metric] = {
                            'mean': pattern_data[metric].mean(),
                            'std': pattern_data[metric].std(),
                            'count': len(pattern_data)
                        }
                pattern_stats[pattern_name] = stats_dict

        patterns['sequence_patterns'] = pattern_stats

        # æ•°å€¼åˆ†å¸ƒåˆ†æ
        value_analysis = {}
        for pos in ['pos1', 'pos2', 'pos3']:
            pos_analysis = {}
            for value in range(1, 10):
                value_data = self.data[self.features[pos] == value]
                if len(value_data) > 0:
                    value_stats = {}
                    for metric in self.target_metrics:
                        if metric in value_data.columns:
                            value_stats[metric] = {
                                'mean': value_data[metric].mean(),
                                'count': len(value_data)
                            }
                    pos_analysis[f'value_{value}'] = value_stats
            value_analysis[pos] = pos_analysis

        patterns['value_distribution'] = value_analysis

        self.results['patterns'] = patterns
        print("âœ… é…ç½®æ¨¡å¼åˆ†æå®Œæˆ")
        return patterns

    def build_prediction_models(self) -> Dict:
        """æ„å»ºé¢„æµ‹æ¨¡å‹"""
        print("ğŸ¤– æ„å»ºé¢„æµ‹æ¨¡å‹...")

        models = {}

        for metric in self.target_metrics:
            if metric not in self.data.columns:
                continue

            # æ•°æ®å‡†å¤‡
            X = self.features
            y = self.data[metric]

            # è®­ç»ƒæµ‹è¯•åˆ†å‰²
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42
            )

            # æ•°æ®æ ‡å‡†åŒ–
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)

            # éšæœºæ£®æ—æ¨¡å‹
            rf_model = RandomForestRegressor(
                n_estimators=100,
                random_state=42,
                n_jobs=-1
            )

            rf_model.fit(X_train_scaled, y_train)

            # æ¨¡å‹è¯„ä¼°
            y_pred = rf_model.predict(X_test_scaled)
            r2 = r2_score(y_test, y_pred)
            rmse = np.sqrt(mean_squared_error(y_test, y_pred))

            # ç‰¹å¾é‡è¦æ€§
            feature_importance = dict(zip(X.columns, rf_model.feature_importances_))

            models[metric] = {
                'model': rf_model,
                'scaler': scaler,
                'r2_score': r2,
                'rmse': rmse,
                'feature_importance': feature_importance
            }

        self.results['models'] = models
        print(f"âœ… é¢„æµ‹æ¨¡å‹æ„å»ºå®Œæˆï¼Œå…±{len(models)}ä¸ªæ¨¡å‹")
        return models

    def run_complete_analysis(self, output_dir: str = None) -> Dict:
        """è¿è¡Œå‡çº§ç‰ˆå®Œæ•´åˆ†ææµç¨‹"""
        print("ğŸš€ å¼€å§‹æ·±åº¦ä½“éªŒé…ç½®å½±å“åˆ†æ...")

        # æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
        self.load_and_preprocess()

        # åŸºç¡€åˆ†æ
        correlations = self.correlation_analysis()
        patterns = self.pattern_analysis()

        # æ·±åº¦åˆ†æï¼ˆæ–°å¢ï¼‰
        independent_effects = self.position_independent_analysis()
        interaction_effects = self.interaction_analysis()
        dynamic_effects = self.dynamic_impact_analysis()
        mechanism_effects = self.mechanism_analysis()

        # æ–°å¢æ·±åº¦åˆ†ææ–¹æ³•
        value_specific_effects = self.value_specific_analysis()
        gradient_effects = self.gradient_effect_analysis()
        dock_deep_effects = self.dock_sequence_deep_analysis()
        pressure_dynamics = self.pressure_dynamics_analysis()

        # æœºå™¨å­¦ä¹ å»ºæ¨¡
        models = self.build_prediction_models()

        # åˆ›å»ºå¢å¼ºå¯è§†åŒ–
        self.create_enhanced_visualizations(output_dir)

        # ç”Ÿæˆæ·±åº¦æŠ¥å‘Š
        report_path = self.generate_enhanced_report()

        # è¾“å‡ºå…³é”®å‘ç°
        self._print_key_findings()

        # æ±‡æ€»ç»“æœ
        summary = {
            'data_summary': {
                'total_records': len(self.data),
                'unique_configs': len(self.data[['pos1', 'pos2', 'pos3']].drop_duplicates()),
                'target_metrics': len(self.target_metrics)
            },
            'analysis_results': {
                'basic_analysis': len(correlations) > 0 and len(patterns) > 0,
                'advanced_analysis': len(independent_effects) > 0 and len(interaction_effects) > 0,
                'dynamic_analysis': len(dynamic_effects) > 0,
                'mechanism_analysis': len(mechanism_effects) > 0,
                'value_specific_analysis': len(value_specific_effects) > 0,
                'gradient_analysis': len(gradient_effects) > 0,
                'dock_deep_analysis': len(dock_deep_effects) > 0,
                'pressure_dynamics': len(pressure_dynamics) > 0,
                'models_completed': len(models) > 0,
                'best_model_r2': max([m['r2_score'] for m in models.values()]) if models else 0
            },
            'output_files': {
                'report_path': report_path,
                'charts_directory': output_dir or (
                    Path(self.csv_path).parent / "analysis_charts" if self.csv_path
                    else Path(self.csv_directory) / "analysis_output"
                )
            }
        }

        print("ğŸ‰ æ·±åº¦åˆ†ææµç¨‹æ‰§è¡Œå®Œæˆ!")
        print(f"ğŸ“„ æŠ¥å‘Šè·¯å¾„: {summary['output_files']['report_path']}")
        print(f"ğŸ“Š å›¾è¡¨ç›®å½•: {summary['output_files']['charts_directory']}")

        return summary

    def create_enhanced_visualizations(self, output_dir: str = None):
        """åˆ›å»ºå¢å¼ºå¯è§†åŒ–å›¾è¡¨"""
        print("ğŸ“Š åˆ›å»ºå¢å¼ºå¯è§†åŒ–å›¾è¡¨...")

        if output_dir is None:
            # å¤šæ–‡ä»¶æ¨¡å¼ä½¿ç”¨csv_directoryï¼Œå•æ–‡ä»¶æ¨¡å¼ä½¿ç”¨csv_path
            if self.csv_path:
                output_dir = Path(self.csv_path).parent / "analysis_charts"
            else:
                output_dir = Path(self.csv_directory) / "analysis_output"

        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)

        # å®‰å…¨å›¾è¡¨ç»˜åˆ¶åˆ—è¡¨
        charts = [
            ("ä½ç½®ç›¸å…³æ€§çƒ­åŠ›å›¾", self._plot_position_correlation_heatmap),
            ("é…ç½®æ¨¡å¼ç®±çº¿å›¾", self._plot_pattern_boxplots),
            ("ç‰¹å¾é‡è¦æ€§å›¾", self._plot_feature_importance),
            ("é…ç½®åˆ†å¸ƒå›¾", self._plot_config_distribution),
            ("æ¨¡å‹æ€§èƒ½å›¾", self._plot_model_performance),
            ("ä½ç½®ç‹¬ç«‹æ•ˆåº”å›¾", self._plot_independent_effects),
            ("äº¤äº’æ•ˆåº”å›¾", self._plot_interaction_effects),
            ("æœºåˆ¶åˆ†æå›¾", self._plot_mechanism_analysis),
            ("æ•°å€¼å½±å“çŸ©é˜µçƒ­åŠ›å›¾", self._plot_value_impact_heatmaps),
            ("æ•°å€¼æ¢¯åº¦æ•ˆåº”æ›²çº¿", self._plot_gradient_curves),
            ("Dockåºåˆ—æ¨¡å¼å›¾", self._plot_dock_sequence_patterns),
            ("å‹åŠ›åŠ¨æ€åˆ†å¸ƒå›¾", self._plot_pressure_dynamics)
        ]

        successful_charts = 0
        for chart_name, chart_func in charts:
            try:
                print(f"   ç»˜åˆ¶ {chart_name}...")
                chart_func(output_path)
                successful_charts += 1
                print(f"   âœ… {chart_name} å®Œæˆ")
            except Exception as e:
                print(f"   âŒ {chart_name} å¤±è´¥: {str(e)}")
                continue

        print(f"âœ… å¢å¼ºå¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {output_path}")
        print(f"ğŸ“Š æˆåŠŸç»˜åˆ¶: {successful_charts}/{len(charts)} ä¸ªå›¾è¡¨")

    def _plot_independent_effects(self, output_path: Path):
        """ç»˜åˆ¶ä½ç½®ç‹¬ç«‹æ•ˆåº”å›¾"""
        if 'independent_effects' not in self.results:
            return

        fig, axes = plt.subplots(1, 3, figsize=(18, 6))

        for i, pos in enumerate(['pos1', 'pos2', 'pos3']):
            if pos in self.results['independent_effects']:
                effects = self.results['independent_effects'][pos]

                if 'DifficultyScore' in effects:
                    marginal_data = effects['DifficultyScore']['marginal_effects']
                    if marginal_data:
                        x_vals, y_vals = zip(*marginal_data)
                        axes[i].plot(x_vals, y_vals, 'o-', linewidth=2, markersize=8)
                        axes[i].set_title(f'{pos}å¯¹DifficultyScoreçš„è¾¹é™…æ•ˆåº”')
                        axes[i].set_xlabel(f'{pos}æ•°å€¼')
                        axes[i].set_ylabel('DifficultyScoreå‡å€¼')
                        axes[i].grid(True, alpha=0.3)

        plt.suptitle('ä½ç½®ç‹¬ç«‹æ•ˆåº”åˆ†æ', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(output_path / 'independent_effects.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_interaction_effects(self, output_path: Path):
        """ç»˜åˆ¶äº¤äº’æ•ˆåº”å¼ºåº¦å›¾ï¼Œå¢åŠ æ•°å€¼èŒƒå›´æ£€æŸ¥"""
        if 'interaction_effects' not in self.results:
            return

        pairs = list(self.results['interaction_effects'].keys())
        metrics = ['DifficultyScore', 'PeakDockCount']

        if not pairs:
            print("   âš ï¸ æ— äº¤äº’æ•ˆåº”æ•°æ®ï¼Œè·³è¿‡ç»˜åˆ¶")
            return

        # æ”¶é›†æ‰€æœ‰æ•°å€¼å¹¶æ£€æŸ¥èŒƒå›´
        all_strengths = []
        for pair in pairs:
            for metric in metrics:
                if metric in self.results['interaction_effects'][pair]:
                    strength = self.results['interaction_effects'][pair][metric]['interaction_strength']
                    if np.isfinite(strength):  # æ£€æŸ¥æ•°å€¼æœ‰æ•ˆæ€§
                        all_strengths.append(abs(strength))

        if not all_strengths:
            print("   âš ï¸ æ— æœ‰æ•ˆäº¤äº’æ•ˆåº”å¼ºåº¦æ•°æ®ï¼Œè·³è¿‡ç»˜åˆ¶")
            return

        # æ£€æŸ¥æ•°å€¼èŒƒå›´åˆç†æ€§
        max_strength = max(all_strengths)
        if max_strength > 1e6:  # å¼‚å¸¸å¤§æ•°å€¼
            print(f"   âš ï¸ æ£€æµ‹åˆ°å¼‚å¸¸å¤§æ•°å€¼({max_strength:.2e})ï¼Œè·³è¿‡äº¤äº’æ•ˆåº”å›¾ç»˜åˆ¶")
            return

        try:
            fig, ax = plt.subplots(figsize=(min(12, len(pairs) * 2), 6))

            x_pos = np.arange(len(pairs))
            width = 0.35

            for i, metric in enumerate(metrics):
                strengths = []
                for pair in pairs:
                    if metric in self.results['interaction_effects'][pair]:
                        strength = self.results['interaction_effects'][pair][metric]['interaction_strength']
                        # æ•°å€¼å®‰å…¨æ£€æŸ¥
                        if np.isfinite(strength):
                            strengths.append(max(0, min(1, abs(strength))))  # é™åˆ¶åœ¨[0,1]èŒƒå›´
                        else:
                            strengths.append(0)
                    else:
                        strengths.append(0)

                if any(s > 0 for s in strengths):  # åªç»˜åˆ¶æœ‰æ•°æ®çš„æŒ‡æ ‡
                    bars = ax.bar(x_pos + i * width, strengths, width, label=metric, alpha=0.7)

                    # æ·»åŠ æ•°å€¼æ ‡ç­¾
                    for bar, strength in zip(bars, strengths):
                        if strength > 0.001:  # åªæ˜¾ç¤ºæ˜¾è‘—å€¼
                            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max_strength*0.01,
                                   f'{strength:.3f}', ha='center', va='bottom', fontsize=9)

            ax.set_xlabel('ä½ç½®ç»„åˆ')
            ax.set_ylabel('äº¤äº’æ•ˆåº”å¼ºåº¦')
            ax.set_title('ä½ç½®é—´äº¤äº’æ•ˆåº”åˆ†æ')
            ax.set_xticks(x_pos + width / 2)
            ax.set_xticklabels(pairs, rotation=45 if len(pairs) > 3 else 0)
            ax.legend()
            ax.grid(True, alpha=0.3)

            # è®¾ç½®åˆç†çš„Yè½´èŒƒå›´
            ax.set_ylim(0, max(all_strengths) * 1.1)

            plt.tight_layout()
            plt.savefig(output_path / 'interaction_effects.png', dpi=300, bbox_inches='tight')
            plt.close()

        except Exception as e:
            print(f"   âŒ äº¤äº’æ•ˆåº”å›¾ç»˜åˆ¶å†…éƒ¨é”™è¯¯: {str(e)}")
            plt.close('all')  # ç¡®ä¿æ¸…ç†èµ„æº

    def _plot_mechanism_analysis(self, output_path: Path):
        """ç»˜åˆ¶æœºåˆ¶åˆ†æå›¾"""
        if 'mechanism_effects' not in self.results:
            return

        fig, axes = plt.subplots(1, 3, figsize=(18, 6))

        for i, pos in enumerate(['pos1', 'pos2', 'pos3']):
            if pos in self.results['mechanism_effects']:
                mechanism = self.results['mechanism_effects'][pos]

                if 'mediation_effects' in mechanism:
                    mediators = list(mechanism['mediation_effects'].keys())
                    strengths = [mechanism['mediation_effects'][m]['mediation_strength']
                               for m in mediators]

                    colors = ['red' if s > 0 else 'blue' for s in strengths]
                    bars = axes[i].barh(mediators, [abs(s) for s in strengths], color=colors, alpha=0.7)

                    axes[i].set_title(f'{pos}çš„ä¸­ä»‹æ•ˆåº”åˆ†æ')
                    axes[i].set_xlabel('ä¸­ä»‹æ•ˆåº”å¼ºåº¦')

                    # æ·»åŠ æ•°å€¼æ ‡ç­¾
                    for bar, strength in zip(bars, strengths):
                        axes[i].text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,
                                   f'{strength:.3f}', va='center', fontsize=9)

        plt.suptitle('å½±å“æœºåˆ¶ä¸­ä»‹è·¯å¾„åˆ†æ', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(output_path / 'mechanism_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()

    def generate_enhanced_report(self, output_path: str = None) -> str:
        """ç”Ÿæˆå¢å¼ºç‰ˆåˆ†ææŠ¥å‘Š"""
        print("ğŸ“‹ ç”Ÿæˆæ·±åº¦åˆ†ææŠ¥å‘Š...")

        if output_path is None:
            # å¤šæ–‡ä»¶æ¨¡å¼ä½¿ç”¨csv_directoryï¼Œå•æ–‡ä»¶æ¨¡å¼ä½¿ç”¨csv_path
            if self.csv_path:
                output_path = Path(self.csv_path).parent / "enhanced_analysis_report.md"
            else:
                output_path = Path(self.csv_directory) / "enhanced_analysis_report.md"

        report = []
        report.append("# ä½“éªŒæ¨¡å¼é…ç½®[x,y,z]æ·±åº¦å½±å“åˆ†ææŠ¥å‘Š\n")

        # æ•°æ®æºä¿¡æ¯
        if self.csv_path:
            report.append(f"**æ•°æ®æº**: {self.csv_path}")
        else:
            report.append(f"**æ•°æ®æº**: {self.csv_directory} ({len(self.csv_files)}ä¸ªCSVæ–‡ä»¶)")

        report.append(f"**åˆ†ææ—¶é—´**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"**æ ·æœ¬æ•°é‡**: {len(self.data):,}æ¡\n")

        # æ•°æ®æ¦‚è§ˆ
        self._add_data_overview(report)

        # ä½ç½®ç‹¬ç«‹æ•ˆåº”åˆ†æ
        self._add_independent_effects_report(report)

        # äº¤äº’æ•ˆåº”åˆ†æ
        self._add_interaction_effects_report(report)

        # åŠ¨æ€å½±å“åˆ†æ
        self._add_dynamic_effects_report(report)

        # æœºåˆ¶åˆ†æ
        self._add_mechanism_effects_report(report)

        # æ–°å¢æ·±åº¦åˆ†ææŠ¥å‘Š
        self._add_value_specific_report(report)
        self._add_gradient_effects_report(report)
        self._add_dock_sequence_report(report)
        self._add_pressure_dynamics_report(report)

        # å…³é”®å‘ç°ä¸å»ºè®®
        self._add_key_findings_and_recommendations(report)

        # å†™å…¥æ–‡ä»¶
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report))

        print(f"âœ… æ·±åº¦åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
        return str(output_path)
    

    def _add_data_overview(self, report):
        report.append("## ğŸ“Š æ•°æ®æ¦‚è§ˆ\n")
        report.append(f"- é…ç½®èŒƒå›´: pos1=[{self.features['pos1'].min()}-{self.features['pos1'].max()}], " +
                     f"pos2=[{self.features['pos2'].min()}-{self.features['pos2'].max()}], " +
                     f"pos3=[{self.features['pos3'].min()}-{self.features['pos3'].max()}]")
        report.append(f"- å”¯ä¸€é…ç½®æ•°: {len(self.data[['pos1', 'pos2', 'pos3']].drop_duplicates())}ç§")

    def _add_independent_effects_report(self, report):
        if 'independent_effects' not in self.results:
            return

        report.append("\n## ğŸ¯ ä½ç½®ç‹¬ç«‹æ•ˆåº”åˆ†æ\n")

        for pos in ['pos1', 'pos2', 'pos3']:
            if pos in self.results['independent_effects']:
                effects = self.results['independent_effects'][pos]
                report.append(f"### {pos}çš„ç‹¬ç«‹æ•ˆåº”:")

                for metric, data in effects.items():
                    if 'avg_marginal_diff' in data:
                        avg_diff = data['avg_marginal_diff']
                        volatility = data['marginal_volatility']
                        report.append(f"- **{metric}**: å¹³å‡è¾¹é™…æ•ˆåº” {avg_diff:.3f}, æ³¢åŠ¨æ€§ {volatility:.3f}")

    def _add_interaction_effects_report(self, report):
        if 'interaction_effects' not in self.results:
            return

        report.append("\n## ğŸ”„ ä½ç½®äº¤äº’æ•ˆåº”åˆ†æ\n")

        for pair, effects in self.results['interaction_effects'].items():
            report.append(f"### {pair}äº¤äº’æ•ˆåº”:")
            for metric, data in effects.items():
                gain = data['interaction_gain']
                strength = data['interaction_strength']
                report.append(f"- **{metric}**: äº¤äº’å¢ç›Š {gain:.4f}, æ•ˆåº”å¼ºåº¦ {strength:.4f}")

    def _add_dynamic_effects_report(self, report):
        if 'dynamic_effects' not in self.results:
            return

        report.append("\n## â±ï¸ åŠ¨æ€å½±å“åˆ†æ\n")

        for pos, dynamics in self.results['dynamic_effects'].items():
            report.append(f"### {pos}çš„æ—¶åºå½±å“:")
            for stage, corr in dynamics.items():
                stage_name = {'early_correlation': 'å‰æœŸ', 'middle_correlation': 'ä¸­æœŸ', 'late_correlation': 'åæœŸ'}.get(stage, stage)
                report.append(f"- **{stage_name}**: ç›¸å…³æ€§ {corr:.3f}")

    def _add_mechanism_effects_report(self, report):
        if 'mechanism_effects' not in self.results:
            return

        report.append("\n## ğŸ” å½±å“æœºåˆ¶åˆ†æ\n")

        for pos, mechanism in self.results['mechanism_effects'].items():
            report.append(f"### {pos}çš„å½±å“æœºåˆ¶:")
            if 'direct_effect' in mechanism:
                report.append(f"- **ç›´æ¥æ•ˆåº”**: {mechanism['direct_effect']:.3f}")

            if 'strongest_mediation_path' in mechanism:
                strongest = mechanism['strongest_mediation_path']
                report.append(f"- **æœ€å¼ºä¸­ä»‹è·¯å¾„**: {strongest['mediator']} (strength: {strongest['strength']:.3f})")

    def _add_key_findings_and_recommendations(self, report):
        report.append("\n## ğŸ’¡ å…³é”®å‘ç°ä¸å»ºè®®\n")

        # ä½ç½®é‡è¦æ€§æ’åº
        if 'correlations' in self.results:
            pos_importance = {}
            for pos in ['pos1', 'pos2', 'pos3']:
                avg_abs_corr = np.mean([
                    abs(self.results['correlations']['position_correlations'][pos][m]['correlation'])
                    for m in ['DifficultyScore', 'PeakDockCount', 'PressureValueMean']
                    if m in self.results['correlations']['position_correlations'][pos]
                ])
                pos_importance[pos] = avg_abs_corr

            sorted_positions = sorted(pos_importance.items(), key=lambda x: x[1], reverse=True)
            report.append("### ä¸»è¦å‘ç°:")
            report.append(f"1. **ä½ç½®é‡è¦æ€§æ’åº**: {' > '.join([f'{p}({v:.3f})' for p, v in sorted_positions])}")

        # å…³é”®äº¤äº’æ•ˆåº”
        if 'interaction_effects' in self.results:
            max_interaction = None
            max_strength = 0
            for pair, effects in self.results['interaction_effects'].items():
                for metric, data in effects.items():
                    if data['interaction_strength'] > max_strength:
                        max_strength = data['interaction_strength']
                        max_interaction = f"{pair}->{metric}"
            if max_interaction:
                report.append(f"2. **æœ€å…³é”®äº¤äº’æ•ˆåº”**: {max_interaction} (strength: {max_strength:.3f})")

        report.append("\n### ä¼˜åŒ–å»ºè®®:")
        report.append("1. é‡ç‚¹å…³æ³¨å½±å“åŠ›æœ€å¤§çš„ä½ç½®å‚æ•°")
        report.append("2. è€ƒè™‘ä½ç½®é—´çš„äº¤äº’æ•ˆåº”ï¼Œé¿å…å•çº¯çš„ç‹¬ç«‹è°ƒæ•´")
        report.append("3. æ ¹æ®ä¸­ä»‹æœºåˆ¶é’ˆå¯¹æ€§ä¼˜åŒ–ï¼Œæé«˜è°ƒæ•´ç²¾åº¦")

    def _add_value_specific_report(self, report):
        """æ·»åŠ å•ä¸€æ•°å€¼æ·±åº¦åˆ†ææŠ¥å‘Š"""
        if 'value_specific_effects' not in self.results:
            return

        report.append("\n## ğŸ¯ æ•°å€¼ç‰¹å¼‚æ€§å½±å“åˆ†æ\n")

        # é‡ç‚¹åˆ†æå‡ ä¸ªå…³é”®æ•°å€¼
        key_values = [1, 3, 5, 7, 9]  # åˆ†æå…³é”®æ•°å€¼ç‚¹

        for value in key_values:
            value_key = f"value_{value}"
            if value_key in self.results['value_specific_effects']:
                report.append(f"### æ•°å€¼{value}çš„å½±å“ç‰¹å¾:")

                value_data = self.results['value_specific_effects'][value_key]

                for pos in ['pos1', 'pos2', 'pos3']:
                    if pos in value_data:
                        pos_data = value_data[pos]
                        report.append(f"#### {pos}ä½ç½®:")

                        # éš¾åº¦å½±å“
                        if 'difficulty_impact' in pos_data:
                            diff_data = pos_data['difficulty_impact']
                            report.append(f"- **éš¾åº¦å½±å“**: å¹³å‡{diff_data['mean']:.2f}, æ ‡å‡†å·®{diff_data['std']:.2f}, æ ·æœ¬{diff_data['count']}ä¸ª")

                        # èƒœç‡å½±å“
                        if 'win_rate' in pos_data:
                            win_data = pos_data['win_rate']
                            report.append(f"- **èƒœç‡è¡¨ç°**: æˆåŠŸç‡{win_data['success_rate']:.3f} ({win_data['total_games']}å±€æ¸¸æˆ)")

                        # Dockå½±å“
                        if 'dock_impact' in pos_data:
                            dock_data = pos_data['dock_impact']
                            if 'avg_sequence_length' in dock_data:
                                report.append(f"- **æ¸¸æˆæ—¶é•¿**: å¹³å‡{dock_data['avg_sequence_length']:.1f}æ­¥")

                        # å‹åŠ›å½±å“
                        if 'pressure_impact' in pos_data:
                            pressure_data = pos_data['pressure_impact']
                            for pressure_type, pressure_stats in pressure_data.items():
                                if 'mean' in pressure_stats:
                                    report.append(f"- **{pressure_type}**: {pressure_stats['mean']:.3f}")

                report.append("")  # ç©ºè¡Œåˆ†éš”

    def _add_gradient_effects_report(self, report):
        """æ·»åŠ æ•°å€¼æ¢¯åº¦æ•ˆåº”æŠ¥å‘Š"""
        if 'gradient_effects' not in self.results:
            return

        report.append("\n## ğŸ“ˆ æ•°å€¼æ¢¯åº¦æ•ˆåº”åˆ†æ\n")

        for pos in ['pos1', 'pos2', 'pos3']:
            if pos in self.results['gradient_effects']:
                report.append(f"### {pos}æ¢¯åº¦æ•ˆåº”:")
                pos_gradients = self.results['gradient_effects'][pos]

                for metric in ['DifficultyScore', 'PeakDockCount', 'PressureValueMean']:
                    if metric in pos_gradients:
                        gradient_data = pos_gradients[metric]
                        critical_point = gradient_data['critical_point']
                        max_gradient = gradient_data['max_gradient']
                        avg_gradient = gradient_data['avg_gradient']

                        report.append(f"- **{metric}æ¢¯åº¦**: å¹³å‡æ¢¯åº¦{avg_gradient:.3f}, æœ€å¤§æ¢¯åº¦{max_gradient:.3f}")
                        if critical_point:
                            report.append(f"  - å…³é”®è½¬æŠ˜ç‚¹: æ•°å€¼{critical_point}")

                report.append("")

    def _add_dock_sequence_report(self, report):
        """æ·»åŠ Dockåºåˆ—æ·±åº¦åˆ†ææŠ¥å‘Š"""
        if 'dock_deep_analysis' not in self.results:
            return

        report.append("\n## ğŸš¢ Dockåºåˆ—æ·±åº¦åˆ†æ\n")

        for pos in ['pos1', 'pos2', 'pos3']:
            if pos in self.results['dock_deep_analysis']:
                report.append(f"### {pos}åºåˆ—ç‰¹å¾:")
                pos_data = self.results['dock_deep_analysis'][pos]

                # æ‰¾å‡ºæœ€é«˜å’Œæœ€ä½èƒœç‡çš„é…ç½®
                best_config = None
                worst_config = None
                best_rate = 0
                worst_rate = 1

                for value in range(1, 10):
                    value_key = f'value_{value}'
                    if value_key in pos_data:
                        success_rate = pos_data[value_key]['success_rate']
                        if success_rate > best_rate:
                            best_rate = success_rate
                            best_config = value
                        if success_rate < worst_rate:
                            worst_rate = success_rate
                            worst_config = value

                if best_config:
                    report.append(f"- **æœ€ä½³é…ç½®**: æ•°å€¼{best_config}, èƒœç‡{best_rate:.3f}")
                if worst_config:
                    report.append(f"- **æœ€å·®é…ç½®**: æ•°å€¼{worst_config}, èƒœç‡{worst_rate:.3f}")

                # åˆ†æåºåˆ—æ¨¡å¼
                for value in range(1, 10):
                    value_key = f'value_{value}'
                    if value_key in pos_data:
                        value_analysis = pos_data[value_key]
                        if 'patterns' in value_analysis and 'pattern_type' in value_analysis['patterns']:
                            pattern_type = value_analysis['patterns']['pattern_type']
                            avg_length = value_analysis['avg_length']

                            if pattern_type != 'insufficient_data':
                                pattern_name = {
                                    'increasing_pressure': 'å‹åŠ›é€’å¢å‹',
                                    'decreasing_pressure': 'å‹åŠ›é€’å‡å‹',
                                    'stable_pressure': 'å‹åŠ›ç¨³å®šå‹'
                                }.get(pattern_type, pattern_type)

                                report.append(f"- **æ•°å€¼{value}**: {pattern_name}, å¹³å‡æ—¶é•¿{avg_length:.1f}æ­¥")

                report.append("")

    def _add_pressure_dynamics_report(self, report):
        """æ·»åŠ å‹åŠ›åŠ¨æ€åˆ†ææŠ¥å‘Š"""
        if 'pressure_dynamics' not in self.results:
            return

        report.append("\n## âš¡ å‹åŠ›åŠ¨æ€åˆ†æ\n")

        pressure_names = {
            'PressureValueMean': 'å¹³å‡å‹åŠ›',
            'PressureValueMax': 'å³°å€¼å‹åŠ›',
            'PressureValueStdDev': 'å‹åŠ›æ³¢åŠ¨'
        }

        for pos in ['pos1', 'pos2', 'pos3']:
            if pos in self.results['pressure_dynamics']:
                report.append(f"### {pos}å‹åŠ›åŠ¨æ€:")
                pos_data = self.results['pressure_dynamics'][pos]

                # åˆ†ææ¯ç§å‹åŠ›ç±»å‹
                for pressure_type, pressure_name in pressure_names.items():
                    report.append(f"#### {pressure_name}:")

                    # æ‰¾å‡ºæå€¼é…ç½®
                    min_pressure_config = None
                    max_pressure_config = None
                    min_pressure = float('inf')
                    max_pressure = 0

                    for value in range(1, 10):
                        value_key = f'value_{value}'
                        if (value_key in pos_data and
                            pressure_type in pos_data[value_key] and
                            'statistics' in pos_data[value_key][pressure_type]):

                            stats = pos_data[value_key][pressure_type]['statistics']
                            mean_pressure = stats['mean']

                            if mean_pressure < min_pressure:
                                min_pressure = mean_pressure
                                min_pressure_config = value
                            if mean_pressure > max_pressure:
                                max_pressure = mean_pressure
                                max_pressure_config = value

                    if min_pressure_config:
                        report.append(f"- **æœ€ä½å‹åŠ›**: æ•°å€¼{min_pressure_config}, {pressure_name}{min_pressure:.3f}")
                    if max_pressure_config:
                        report.append(f"- **æœ€é«˜å‹åŠ›**: æ•°å€¼{max_pressure_config}, {pressure_name}{max_pressure:.3f}")

                report.append("")

    def _print_key_findings(self):
        """è¾“å‡ºå…³é”®å‘ç°æ‘˜è¦"""
        print("\nğŸ’¡ === å…³é”®å‘ç°æ‘˜è¦ ===")

        # ä½ç½®é‡è¦æ€§æ’åº
        if 'correlations' in self.results:
            pos_importance = {}
            for pos in ['pos1', 'pos2', 'pos3']:
                avg_abs_corr = np.mean([
                    abs(self.results['correlations']['position_correlations'][pos][m]['correlation'])
                    for m in ['DifficultyScore', 'PeakDockCount', 'PressureValueMean']
                    if m in self.results['correlations']['position_correlations'][pos]
                ])
                pos_importance[pos] = avg_abs_corr

            sorted_positions = sorted(pos_importance.items(), key=lambda x: x[1], reverse=True)
            print(f"ğŸ¯ ä½ç½®é‡è¦æ€§æ’åº: {' > '.join([f'{p}({v:.3f})' for p, v in sorted_positions])}")

        # æœ€å¼ºäº¤äº’æ•ˆåº”
        if 'interaction_effects' in self.results:
            max_interaction = None
            max_strength = 0
            for pair, effects in self.results['interaction_effects'].items():
                for metric, data in effects.items():
                    if data['interaction_strength'] > max_strength:
                        max_strength = data['interaction_strength']
                        max_interaction = f"{pair}->{metric}"
            if max_interaction:
                print(f"ğŸ”„ æœ€å¼ºäº¤äº’æ•ˆåº”: {max_interaction} (strength: {max_strength:.3f})")

        # æœ€å…³é”®ä¸­ä»‹è·¯å¾„
        if 'mechanism_effects' in self.results:
            strongest_mediation = None
            max_mediation_strength = 0
            for pos, mechanism in self.results['mechanism_effects'].items():
                if 'strongest_mediation_path' in mechanism:
                    strength = abs(mechanism['strongest_mediation_path']['strength'])
                    if strength > max_mediation_strength:
                        max_mediation_strength = strength
                        strongest_mediation = f"{pos}->{mechanism['strongest_mediation_path']['mediator']}->DifficultyScore"
            if strongest_mediation:
                print(f"ğŸ” æœ€å…³é”®ä¸­ä»‹è·¯å¾„: {strongest_mediation} (strength: {max_mediation_strength:.3f})")

    # åŸºç¡€å¯è§†åŒ–æ–¹æ³•ï¼ˆç®€åŒ–ç‰ˆå®ç°ï¼‰
    def _plot_position_correlation_heatmap(self, output_path: Path):
        """ç»˜åˆ¶ä½ç½®ç›¸å…³æ€§çƒ­åŠ›å›¾"""
        if 'correlations' not in self.results:
            return

        fig, ax = plt.subplots(figsize=(10, 8))

        # æ„å»ºç›¸å…³æ€§çŸ©é˜µ
        positions = ['pos1', 'pos2', 'pos3']
        metrics = ['DifficultyScore', 'PeakDockCount', 'PressureValueMean']
        corr_matrix = []

        for pos in positions:
            pos_corrs = []
            for metric in metrics:
                if pos in self.results['correlations']['position_correlations'] and \
                   metric in self.results['correlations']['position_correlations'][pos]:
                    corr = self.results['correlations']['position_correlations'][pos][metric]['correlation']
                    pos_corrs.append(corr)
                else:
                    pos_corrs.append(0)
            corr_matrix.append(pos_corrs)

        corr_matrix = np.array(corr_matrix)

        # ç»˜åˆ¶çƒ­åŠ›å›¾
        im = ax.imshow(corr_matrix, cmap='RdBu_r', vmin=-1, vmax=1, aspect='auto')

        # è®¾ç½®æ ‡ç­¾
        ax.set_xticks(range(len(metrics)))
        ax.set_xticklabels(metrics, rotation=45, ha='right')
        ax.set_yticks(range(len(positions)))
        ax.set_yticklabels(positions)

        # æ·»åŠ é¢œè‰²æ¡
        plt.colorbar(im, ax=ax, label='ç›¸å…³ç³»æ•°')

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i in range(len(positions)):
            for j in range(len(metrics)):
                ax.text(j, i, f'{corr_matrix[i, j]:.3f}',
                       ha='center', va='center', color='white' if abs(corr_matrix[i, j]) > 0.5 else 'black')

        plt.title('ä½ç½®-æŒ‡æ ‡ç›¸å…³æ€§çƒ­åŠ›å›¾')
        plt.tight_layout()
        plt.savefig(output_path / 'position_correlation_heatmap.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_pattern_boxplots(self, output_path: Path):
        """ç»˜åˆ¶é…ç½®æ¨¡å¼ç®±çº¿å›¾"""
        if 'patterns' not in self.results:
            return

        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        patterns = self.results['patterns']['sequence_patterns']

        metrics = ['DifficultyScore', 'PeakDockCount', 'PressureValueMean', 'TotalMoves']

        for idx, metric in enumerate(metrics):
            ax = axes[idx//2, idx%2]
            pattern_names = []
            pattern_means = []
            pattern_stds = []

            for pattern_name, pattern_data in patterns.items():
                if metric in pattern_data and 'mean' in pattern_data[metric]:
                    pattern_names.append(pattern_name)
                    pattern_means.append(pattern_data[metric]['mean'])
                    pattern_stds.append(pattern_data[metric].get('std', 0))

            if pattern_names:
                bars = ax.bar(pattern_names, pattern_means, yerr=pattern_stds, alpha=0.7, capsize=5)
                ax.set_title(f'{metric}çš„é…ç½®æ¨¡å¼å¯¹æ¯”')
                ax.set_ylabel(metric)
                ax.tick_params(axis='x', rotation=45)

                # æ·»åŠ æ•°å€¼æ ‡ç­¾
                for bar, mean in zip(bars, pattern_means):
                    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + bar.get_height()*0.01,
                           f'{mean:.2f}', ha='center', va='bottom')

        plt.suptitle('é…ç½®æ¨¡å¼åˆ†æç®±çº¿å›¾')
        plt.tight_layout()
        plt.savefig(output_path / 'pattern_boxplots.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_feature_importance(self, output_path: Path):
        """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾"""
        if 'models' not in self.results:
            return

        key_metrics = ['DifficultyScore', 'PeakDockCount', 'PressureValueMean']
        fig, axes = plt.subplots(1, len(key_metrics), figsize=(18, 6))

        for i, metric in enumerate(key_metrics):
            if metric in self.results['models']:
                importance = self.results['models'][metric]['feature_importance']

                # å–å‰8ä¸ªé‡è¦ç‰¹å¾
                top_features = dict(sorted(importance.items(), key=lambda x: x[1], reverse=True)[:8])

                features = list(top_features.keys())
                values = list(top_features.values())

                bars = axes[i].barh(features, values, alpha=0.7)
                axes[i].set_title(f'{metric}\nç‰¹å¾é‡è¦æ€§ (RÂ²={self.results["models"][metric]["r2_score"]:.3f})')
                axes[i].set_xlabel('é‡è¦æ€§åˆ†æ•°')

                # é¢œè‰²æ˜ å°„
                colors = plt.cm.viridis(np.linspace(0, 1, len(values)))
                for bar, color in zip(bars, colors):
                    bar.set_color(color)

        plt.suptitle('æœºå™¨å­¦ä¹ æ¨¡å‹ç‰¹å¾é‡è¦æ€§åˆ†æ')
        plt.tight_layout()
        plt.savefig(output_path / 'feature_importance.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_config_distribution(self, output_path: Path):
        """ç»˜åˆ¶é…ç½®åˆ†å¸ƒæ•£ç‚¹å›¾ï¼Œå¢åŠ æ•°å€¼å®‰å…¨æ£€æŸ¥"""
        try:
            # æ£€æŸ¥å¿…è¦æ•°æ®
            if len(self.features) == 0 or len(self.data) == 0:
                print("   âš ï¸ æ— é…ç½®åˆ†å¸ƒæ•°æ®ï¼Œè·³è¿‡ç»˜åˆ¶")
                return

            fig, axes = plt.subplots(2, 2, figsize=(15, 12))

            # 3Dé…ç½®ç©ºé—´æŠ•å½± (pos1 vs pos2ï¼Œé¢œè‰²è¡¨ç¤ºpos3)
            pos1_vals = self.features['pos1'].values
            pos2_vals = self.features['pos2'].values
            pos3_vals = self.features['pos3'].values

            # æ•°å€¼èŒƒå›´æ£€æŸ¥
            if np.any(~np.isfinite(pos1_vals)) or np.any(~np.isfinite(pos2_vals)) or np.any(~np.isfinite(pos3_vals)):
                print("   âš ï¸ é…ç½®æ•°æ®åŒ…å«æ— æ•ˆå€¼ï¼Œä½¿ç”¨æœ‰æ•ˆå­é›†")
                valid_mask = np.isfinite(pos1_vals) & np.isfinite(pos2_vals) & np.isfinite(pos3_vals)
                pos1_vals = pos1_vals[valid_mask]
                pos2_vals = pos2_vals[valid_mask]
                pos3_vals = pos3_vals[valid_mask]

            if len(pos1_vals) > 0:
                scatter = axes[0,0].scatter(pos1_vals, pos2_vals, c=pos3_vals,
                                          cmap='viridis', alpha=0.6, s=1)
                axes[0,0].set_xlabel('ä½ç½®1æ•°å€¼')
                axes[0,0].set_ylabel('ä½ç½®2æ•°å€¼')
                axes[0,0].set_title('é…ç½®ç©ºé—´åˆ†å¸ƒ (é¢œè‰²=pos3)')
                plt.colorbar(scatter, ax=axes[0,0], label='pos3æ•°å€¼')

            # é…ç½®ä¸DifficultyScoreçš„å…³ç³»
            if 'DifficultyScore' in self.data.columns:
                config_sum = self.features['config_sum'].values
                difficulty = self.data['DifficultyScore'].values

                # è¿‡æ»¤æ— æ•ˆå€¼
                valid_mask = np.isfinite(config_sum) & np.isfinite(difficulty)
                if np.any(valid_mask):
                    axes[0,1].scatter(config_sum[valid_mask], difficulty[valid_mask], alpha=0.3, s=1)
                    axes[0,1].set_xlabel('é…ç½®æ€»å’Œ')
                    axes[0,1].set_ylabel('DifficultyScore')
                    axes[0,1].set_title('é…ç½®æ€»å’Œ vs éš¾åº¦åˆ†æ•°')

            # é…ç½®æ ‡å‡†å·®åˆ†å¸ƒ
            config_std = self.features['config_std'].values
            valid_std = config_std[np.isfinite(config_std)]
            if len(valid_std) > 0:
                axes[1,0].hist(valid_std, bins=min(30, len(valid_std)//10), alpha=0.7, edgecolor='black')
                axes[1,0].set_xlabel('é…ç½®æ ‡å‡†å·®')
                axes[1,0].set_ylabel('é¢‘æ¬¡')
                axes[1,0].set_title('é…ç½®æ ‡å‡†å·®åˆ†å¸ƒ')

            # æå€¼é…ç½®æ•ˆåº”åˆ†æ
            if 'DifficultyScore' in self.data.columns:
                extreme_low = self.data[self.features['has_extreme_low'] == 1]
                extreme_high = self.data[self.features['has_extreme_high'] == 1]
                normal = self.data[(self.features['has_extreme_low'] == 0) & (self.features['has_extreme_high'] == 0)]

                if len(extreme_low) > 0 and len(extreme_high) > 0 and len(normal) > 0:
                    means = []
                    categories = []

                    for name, group in [('æä½é…ç½®', extreme_low), ('æ­£å¸¸é…ç½®', normal), ('æé«˜é…ç½®', extreme_high)]:
                        difficulty_vals = group['DifficultyScore'].values
                        valid_vals = difficulty_vals[np.isfinite(difficulty_vals)]
                        if len(valid_vals) > 0:
                            categories.append(name)
                            means.append(np.mean(valid_vals))

                    if means:
                        colors = ['red', 'gray', 'blue'][:len(means)]
                        bars = axes[1,1].bar(categories, means, alpha=0.7, color=colors)
                        axes[1,1].set_ylabel('å¹³å‡DifficultyScore')
                        axes[1,1].set_title('æå€¼é…ç½®æ•ˆåº”åˆ†æ')

                        # æ·»åŠ æ•°å€¼æ ‡ç­¾
                        for bar, mean in zip(bars, means):
                            if np.isfinite(mean):
                                axes[1,1].text(bar.get_x() + bar.get_width()/2,
                                              bar.get_height() + bar.get_height()*0.01,
                                              f'{mean:.2f}', ha='center', va='bottom')

            plt.suptitle('ä½“éªŒé…ç½®åˆ†å¸ƒç‰¹å¾åˆ†æ')
            plt.tight_layout()
            plt.savefig(output_path / 'config_distribution.png', dpi=300, bbox_inches='tight')
            plt.close()

        except Exception as e:
            print(f"   âŒ é…ç½®åˆ†å¸ƒå›¾ç»˜åˆ¶é”™è¯¯: {str(e)}")
            plt.close('all')

    def _plot_model_performance(self, output_path: Path):
        """ç»˜åˆ¶æ¨¡å‹æ€§èƒ½å›¾"""
        if 'models' not in self.results:
            return

        metrics = list(self.results['models'].keys())
        r2_scores = [self.results['models'][m]['r2_score'] for m in metrics]
        rmse_scores = [self.results['models'][m]['rmse'] for m in metrics]

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

        # RÂ²åˆ†æ•°
        bars1 = ax1.bar(metrics, r2_scores, color='skyblue', alpha=0.7)
        ax1.set_title('é¢„æµ‹æ¨¡å‹RÂ²æ€§èƒ½è¯„åˆ†')
        ax1.set_ylabel('RÂ²åˆ†æ•°')
        ax1.tick_params(axis='x', rotation=45)
        ax1.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='åŸºå‡†çº¿(0.5)')
        ax1.legend()

        # ä¸ºæ¯ä¸ªbaræ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, score in zip(bars1, r2_scores):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{score:.3f}', ha='center', va='bottom')

        # RMSEåˆ†æ•°
        bars2 = ax2.bar(metrics, rmse_scores, color='lightcoral', alpha=0.7)
        ax2.set_title('é¢„æµ‹æ¨¡å‹RMSEè¯¯å·®')
        ax2.set_ylabel('RMSE')
        ax2.tick_params(axis='x', rotation=45)

        plt.suptitle('æœºå™¨å­¦ä¹ æ¨¡å‹æ€§èƒ½è¯„ä¼°')
        plt.tight_layout()
        plt.savefig(output_path / 'model_performance.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_value_impact_heatmaps(self, output_path: Path):
        """ç»˜åˆ¶æ•°å€¼å½±å“çŸ©é˜µçƒ­åŠ›å›¾ - 9Ã—3çŸ©é˜µæ˜¾ç¤ºæ¯ä¸ªæ•°å€¼åœ¨æ¯ä¸ªä½ç½®çš„å½±å“"""
        if 'value_specific_effects' not in self.results:
            return

        fig, axes = plt.subplots(1, 3, figsize=(18, 8))

        # ä¸ºæ¯ä¸ªæŒ‡æ ‡åˆ›å»ºçƒ­åŠ›å›¾
        metrics = ['DifficultyScore', 'success_rate', 'PressureValueMean']
        metric_names = ['éš¾åº¦åˆ†æ•°', 'èƒœç‡', 'å¹³å‡å‹åŠ›']

        for idx, (metric_key, metric_name) in enumerate(zip(metrics, metric_names)):
            # æ„å»º9x3çŸ©é˜µ
            matrix = np.zeros((9, 3))

            for value in range(1, 10):
                value_key = f"value_{value}"
                if value_key in self.results['value_specific_effects']:
                    for pos_idx, pos in enumerate(['pos1', 'pos2', 'pos3']):
                        if pos in self.results['value_specific_effects'][value_key]:
                            pos_data = self.results['value_specific_effects'][value_key][pos]

                            if metric_key == 'DifficultyScore' and 'difficulty_impact' in pos_data:
                                matrix[value-1, pos_idx] = pos_data['difficulty_impact']['mean']
                            elif metric_key == 'success_rate' and 'win_rate' in pos_data:
                                matrix[value-1, pos_idx] = pos_data['win_rate']['success_rate']
                            elif metric_key == 'PressureValueMean' and 'pressure_impact' in pos_data:
                                pressure_data = pos_data['pressure_impact']
                                if 'PressureValueMean' in pressure_data:
                                    matrix[value-1, pos_idx] = pressure_data['PressureValueMean']['mean']

            # ç»˜åˆ¶çƒ­åŠ›å›¾
            im = axes[idx].imshow(matrix, aspect='auto', cmap='RdYlBu_r')

            # è®¾ç½®æ ‡ç­¾
            axes[idx].set_xticks(range(3))
            axes[idx].set_xticklabels(['ä½ç½®1', 'ä½ç½®2', 'ä½ç½®3'])
            axes[idx].set_yticks(range(9))
            axes[idx].set_yticklabels([f'æ•°å€¼{i}' for i in range(1, 10)])
            axes[idx].set_title(f'{metric_name}å½±å“çŸ©é˜µ')

            # æ·»åŠ é¢œè‰²æ¡
            plt.colorbar(im, ax=axes[idx])

        plt.suptitle('æ•°å€¼-ä½ç½®å½±å“çŸ©é˜µçƒ­åŠ›å›¾', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(output_path / 'value_impact_heatmaps.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_gradient_curves(self, output_path: Path):
        """ç»˜åˆ¶æ•°å€¼æ¢¯åº¦æ•ˆåº”æ›²çº¿"""
        if 'gradient_effects' not in self.results:
            return

        fig, axes = plt.subplots(1, 3, figsize=(18, 6))

        for idx, pos in enumerate(['pos1', 'pos2', 'pos3']):
            if pos in self.results['gradient_effects']:
                pos_gradients = self.results['gradient_effects'][pos]

                for metric in ['DifficultyScore', 'PeakDockCount', 'PressureValueMean']:
                    if metric in pos_gradients:
                        data = pos_gradients[metric]
                        value_means = data['value_means']
                        critical_point = data['critical_point']

                        # è¿‡æ»¤æ‰nanå€¼
                        valid_indices = [i for i, val in enumerate(value_means) if not np.isnan(val)]
                        valid_values = [i+1 for i in valid_indices]
                        valid_means = [value_means[i] for i in valid_indices]

                        if len(valid_values) > 2:
                            axes[idx].plot(valid_values, valid_means, 'o-', label=metric, linewidth=2, markersize=6)

                            # æ ‡è®°ä¸´ç•Œç‚¹
                            if critical_point and critical_point in valid_values:
                                critical_idx = valid_values.index(critical_point)
                                axes[idx].scatter([critical_point], [valid_means[critical_idx]],
                                                s=100, c='red', marker='*', zorder=5)

                axes[idx].set_xlabel('é…ç½®æ•°å€¼')
                axes[idx].set_ylabel('æŒ‡æ ‡å‡å€¼')
                axes[idx].set_title(f'{pos}æ¢¯åº¦æ•ˆåº”æ›²çº¿')
                axes[idx].legend()
                axes[idx].grid(True, alpha=0.3)

        plt.suptitle('æ•°å€¼æ¢¯åº¦æ•ˆåº”åˆ†ææ›²çº¿', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(output_path / 'gradient_curves.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_dock_sequence_patterns(self, output_path: Path):
        """ç»˜åˆ¶Dockåºåˆ—æ¨¡å¼å›¾"""
        if 'dock_deep_analysis' not in self.results:
            return

        fig, axes = plt.subplots(2, 3, figsize=(18, 12))

        # ä¸ºæ¯ä¸ªä½ç½®ç»˜åˆ¶ä¸åŒæ•°å€¼çš„åºåˆ—æ¨¡å¼
        colors = plt.cm.Set3(np.linspace(0, 1, 9))

        for pos_idx, pos in enumerate(['pos1', 'pos2', 'pos3']):
            if pos in self.results['dock_deep_analysis']:
                pos_data = self.results['dock_deep_analysis'][pos]

                # ä¸Šæ–¹å›¾: å¹³å‡åºåˆ—é•¿åº¦
                lengths = []
                success_rates = []
                values = []

                for value in range(1, 10):
                    value_key = f'value_{value}'
                    if value_key in pos_data:
                        lengths.append(pos_data[value_key]['avg_length'])
                        success_rates.append(pos_data[value_key]['success_rate'])
                        values.append(value)

                if lengths:
                    bars = axes[0, pos_idx].bar(values, lengths, color=colors[:len(values)], alpha=0.7)
                    axes[0, pos_idx].set_xlabel('é…ç½®æ•°å€¼')
                    axes[0, pos_idx].set_ylabel('å¹³å‡åºåˆ—é•¿åº¦')
                    axes[0, pos_idx].set_title(f'{pos} - å¹³å‡æ¸¸æˆæ—¶é•¿')

                    # æ·»åŠ æ•°å€¼æ ‡ç­¾
                    for bar, length in zip(bars, lengths):
                        axes[0, pos_idx].text(bar.get_x() + bar.get_width()/2,
                                            bar.get_height() + bar.get_height()*0.01,
                                            f'{length:.1f}', ha='center', va='bottom')

                # ä¸‹æ–¹å›¾: æˆåŠŸç‡
                if success_rates:
                    bars = axes[1, pos_idx].bar(values, success_rates, color=colors[:len(values)], alpha=0.7)
                    axes[1, pos_idx].set_xlabel('é…ç½®æ•°å€¼')
                    axes[1, pos_idx].set_ylabel('æˆåŠŸç‡')
                    axes[1, pos_idx].set_title(f'{pos} - èƒœç‡è¡¨ç°')
                    axes[1, pos_idx].set_ylim(0, 1)

                    # æ·»åŠ æ•°å€¼æ ‡ç­¾
                    for bar, rate in zip(bars, success_rates):
                        axes[1, pos_idx].text(bar.get_x() + bar.get_width()/2,
                                            bar.get_height() + 0.02,
                                            f'{rate:.3f}', ha='center', va='bottom')

        plt.suptitle('Dockåºåˆ—æ¨¡å¼ä¸æˆåŠŸç‡åˆ†æ', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(output_path / 'dock_sequence_patterns.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_pressure_dynamics(self, output_path: Path):
        """ç»˜åˆ¶å‹åŠ›åŠ¨æ€åˆ†å¸ƒå›¾"""
        if 'pressure_dynamics' not in self.results:
            return

        fig, axes = plt.subplots(3, 3, figsize=(18, 15))

        pressure_types = ['PressureValueMean', 'PressureValueMax', 'PressureValueStdDev']
        pressure_names = ['å¹³å‡å‹åŠ›', 'æœ€å¤§å‹åŠ›', 'å‹åŠ›æ³¢åŠ¨']

        for row, (pressure_type, pressure_name) in enumerate(zip(pressure_types, pressure_names)):
            for col, pos in enumerate(['pos1', 'pos2', 'pos3']):
                if pos in self.results['pressure_dynamics']:
                    pos_data = self.results['pressure_dynamics'][pos]

                    # æ”¶é›†æ•°æ®
                    values = []
                    means = []
                    q95s = []

                    for value in range(1, 10):
                        value_key = f'value_{value}'
                        if (value_key in pos_data and
                            pressure_type in pos_data[value_key] and
                            'statistics' in pos_data[value_key][pressure_type]):

                            stats = pos_data[value_key][pressure_type]['statistics']
                            values.append(value)
                            means.append(stats['mean'])
                            q95s.append(stats['q95'])

                    if values:
                        # ç»˜åˆ¶å¹³å‡å€¼çº¿
                        axes[row, col].plot(values, means, 'o-', label='å¹³å‡å€¼', linewidth=2, markersize=6)

                        # ç»˜åˆ¶95%åˆ†ä½æ•°çº¿
                        axes[row, col].plot(values, q95s, 's--', label='95%åˆ†ä½æ•°', alpha=0.7)

                        axes[row, col].set_xlabel('é…ç½®æ•°å€¼')
                        axes[row, col].set_ylabel(pressure_name)
                        axes[row, col].set_title(f'{pos} - {pressure_name}åŠ¨æ€')
                        axes[row, col].legend()
                        axes[row, col].grid(True, alpha=0.3)

        plt.suptitle('å‹åŠ›æŒ‡æ ‡åŠ¨æ€åˆ†æ', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(output_path / 'pressure_dynamics.png', dpi=300, bbox_inches='tight')
        plt.close()

    def create_visualizations(self, output_dir: str = None):
        """å…¼å®¹æ–¹æ³•ï¼šåˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        self.create_enhanced_visualizations(output_dir)

    def generate_report(self, output_path: str = None) -> str:
        """å…¼å®¹æ–¹æ³•ï¼šç”ŸæˆåŸºç¡€åˆ†ææŠ¥å‘Š"""
        return self.generate_enhanced_report(output_path)


def main():
    """ä¸»å‡½æ•° - è¿è¡Œæ·±åº¦åˆ†æ"""
    import sys
    import os

    # è®¾ç½®UTF-8ç¼–ç è¾“å‡º
    if sys.platform.startswith('win'):
        os.system('chcp 65001 >nul 2>&1')
        # è®¾ç½®æ§åˆ¶å°è¾“å‡ºç¼–ç 
        import io
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

    print("ğŸš€ ä½“éªŒæ¨¡å¼é…ç½®æ·±åº¦å½±å“åˆ†æå·¥å…·å¯åŠ¨...")

    try:
        # åˆ›å»ºåˆ†æå™¨å®ä¾‹ - é»˜è®¤ä½¿ç”¨å¤šæ–‡ä»¶æ¨¡å¼
        analyzer = ExperienceConfigAnalyzer()

        # è¿è¡Œæ·±åº¦åˆ†æ
        results = analyzer.run_complete_analysis()

        print(f"\nâœ¨ æ·±åº¦åˆ†æå®Œæˆæ€»ç»“:")
        print(f"   ğŸ“‹ å¤„ç†æ•°æ®: {results['data_summary']['total_records']:,}æ¡è®°å½•")
        print(f"   ğŸ¯ å”¯ä¸€é…ç½®: {results['data_summary']['unique_configs']}ç§")
        print(f"   ğŸ“Š ç›®æ ‡æŒ‡æ ‡: {results['data_summary']['target_metrics']}ä¸ª")
        print(f"   ğŸ¤– æœ€ä½³æ¨¡å‹RÂ²: {results['analysis_results']['best_model_r2']:.3f}")

        analysis_status = results['analysis_results']
        print(f"\nğŸ” åˆ†ææ¨¡å—çŠ¶æ€:")
        print(f"   âœ… åŸºç¡€åˆ†æ: {'å®Œæˆ' if analysis_status['basic_analysis'] else 'æœªå®Œæˆ'}")
        print(f"   âœ… é«˜çº§åˆ†æ: {'å®Œæˆ' if analysis_status['advanced_analysis'] else 'æœªå®Œæˆ'}")
        print(f"   âœ… åŠ¨æ€åˆ†æ: {'å®Œæˆ' if analysis_status['dynamic_analysis'] else 'æœªå®Œæˆ'}")
        print(f"   âœ… æœºåˆ¶åˆ†æ: {'å®Œæˆ' if analysis_status['mechanism_analysis'] else 'æœªå®Œæˆ'}")

        # è¾“å‡ºæ•°æ®æ¥æºä¿¡æ¯
        if hasattr(analyzer, 'csv_files') and analyzer.csv_files:
            print(f"\nğŸ“‚ æ•°æ®æ¥æº: {len(analyzer.csv_files)}ä¸ªCSVæ–‡ä»¶")
            for i, csv_file in enumerate(analyzer.csv_files[:5]):  # æœ€å¤šæ˜¾ç¤ºå‰5ä¸ª
                print(f"   {i+1}. {Path(csv_file).name}")
            if len(analyzer.csv_files) > 5:
                print(f"   ... è¿˜æœ‰{len(analyzer.csv_files)-5}ä¸ªæ–‡ä»¶")

    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        raise


def analyze_specific_directory(directory_path: str):
    """åˆ†ææŒ‡å®šç›®å½•çš„CSVæ–‡ä»¶"""
    print(f"ğŸ¯ åˆ†ææŒ‡å®šç›®å½•: {directory_path}")

    try:
        # åˆ›å»ºåˆ†æå™¨å®ä¾‹æŒ‡å®šç›®å½•
        analyzer = ExperienceConfigAnalyzer(csv_directory=directory_path)

        # è¿è¡Œæ·±åº¦åˆ†æ
        results = analyzer.run_complete_analysis()

        return results

    except Exception as e:
        print(f"âŒ æŒ‡å®šç›®å½•åˆ†æå¤±è´¥: {str(e)}")
        raise


def analyze_single_file(file_path: str):
    """åˆ†æå•ä¸ªCSVæ–‡ä»¶"""
    print(f"ğŸ“„ åˆ†æå•ä¸ªæ–‡ä»¶: {file_path}")

    try:
        # åˆ›å»ºåˆ†æå™¨å®ä¾‹æŒ‡å®šæ–‡ä»¶
        analyzer = ExperienceConfigAnalyzer(csv_path=file_path)

        # è¿è¡Œæ·±åº¦åˆ†æ
        results = analyzer.run_complete_analysis()

        return results

    except Exception as e:
        print(f"âŒ å•æ–‡ä»¶åˆ†æå¤±è´¥: {str(e)}")
        raise


if __name__ == "__main__":
    main()